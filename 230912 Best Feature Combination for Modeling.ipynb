{"cells":[{"cell_type":"code","source":["import pandas as pd\n","\n","data = pd.read_csv('/content/drive/MyDrive/ELM/merged_CSV/merged_132K.csv').dropna()\n","data = data.drop_duplicates(subset=['.geo'])\n","\n","print('Dataset Features :',data.columns.tolist())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vmx28icpMfwX","executionInfo":{"status":"ok","timestamp":1694535682450,"user_tz":-480,"elapsed":6595,"user":{"displayName":"Akib Javed","userId":"01752818298668219500"}},"outputId":"88c8ad8a-520a-4805-bbe5-f31fdb886fe6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset Features : ['system:index', 'City', 'Class', 'HH', 'HH_asm', 'HH_contrast', 'HH_corr', 'HH_dent', 'HH_diss', 'HH_dvar', 'HH_ent', 'HH_idm', 'HH_imcorr1', 'HH_imcorr2', 'HH_inertia', 'HH_maxcorr', 'HH_prom', 'HH_savg', 'HH_sent', 'HH_shade', 'HH_svar', 'HH_var', 'HV', 'HV_asm', 'HV_contrast', 'HV_corr', 'HV_dent', 'HV_diss', 'HV_dvar', 'HV_ent', 'HV_idm', 'HV_imcorr1', 'HV_imcorr2', 'HV_inertia', 'HV_maxcorr', 'HV_prom', 'HV_savg', 'HV_sent', 'HV_shade', 'HV_svar', 'HV_var', 'NSAI1', 'NSAI2', 'SISAI', 'VH', 'VH_asm', 'VH_contrast', 'VH_corr', 'VH_dent', 'VH_diss', 'VH_dvar', 'VH_ent', 'VH_idm', 'VH_imcorr1', 'VH_imcorr2', 'VH_inertia', 'VH_maxcorr', 'VH_prom', 'VH_savg', 'VH_sent', 'VH_shade', 'VH_svar', 'VH_var', 'VV', 'VV_asm', 'VV_contrast', 'VV_corr', 'VV_dent', 'VV_diss', 'VV_dvar', 'VV_ent', 'VV_idm', 'VV_imcorr1', 'VV_imcorr2', 'VV_inertia', 'VV_maxcorr', 'VV_prom', 'VV_savg', 'VV_sent', 'VV_shade', 'VV_svar', 'VV_var', 'indbiMax', 'indbiMedian', 'mndwiMax', 'mndwiMedian', 'mndwiSD', 'ndbiMedian', 'ndbiMin', 'ndbiSD', 'nduiMedian', 'nduiMin', 'nduiSD', 'ndviMax', 'ndviMedian', 'ndviSD', 's2_aerosols', 's2_blue', 's2_green', 's2_nir', 's2_red', 's2_redEdge1', 's2_redEdge2', 's2_redEdge3', 's2_redEdge4', 's2_swir1', 's2_swir2', 's2_waterVapor', 'swiRedMedian', 'viirs', '.geo']\n"]}]},{"cell_type":"code","source":["# CLASSIFIER based Best Feature Combination\n","# Version 14.4 (11 September 2023)\n","# add addtional accuracy matrix for best feature combination model\n","\n","# Import necessary libraries\n","import time\n","\n","# Record the current time\n","start_time = time.time()\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, cohen_kappa_score\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","import xgboost as xgb\n","\n","# Define a function for Accuracy Assessment\n","def evaluate_accuracy(y_true, y_pred):\n","\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average='macro')\n","    recall = recall_score(y_true, y_pred, average='macro')\n","    f1 = f1_score(y_true, y_pred, average='macro')\n","    confusion = confusion_matrix(y_true, y_pred)\n","    report = classification_report(y_true, y_pred)\n","    kappa = cohen_kappa_score(y_true, y_pred)\n","\n","    print(\"\\nAccuracy Metrics:\\n\")\n","\n","    print(f\"Accuracy: {accuracy:.3f}\")\n","    print(f\"Precision: {precision:.3f}\")\n","    print(f\"Recall: {recall:.3f}\")\n","    print(f\"F1 Score: {f1:.3f}\")\n","    print(f\"Kappa: {kappa:.3f}\")\n","\n","    print(\"\\nConfusion Matrix:\")\n","    print(confusion)\n","\n","    print(\"\\nClassification Report:\")\n","    print(report)\n","\n","    return accuracy, precision, recall, f1, kappa, confusion, report\n","\n","\n","# Define a function for Classifier based model training and evaluation\n","def train_and_evaluate_classifier(X_train, X_test, y_train, y_test):\n","    # Define the classifiers\n","    rf_model = RandomForestClassifier(n_estimators=250, random_state=100, n_jobs=-1)\n","    xgb_model = xgb.XGBClassifier(n_estimators=250, random_state=100, n_jobs=-1)\n","\n","    # Create a VotingClassifier with soft voting\n","    ensemble_model = VotingClassifier(\n","        estimators=[('rf', rf_model), ('xgb', xgb_model)],\n","        voting='soft'\n","    ).fit(X_train, y_train)\n","\n","    # Make predictions and calculate accuracy\n","    predictions = ensemble_model.predict(X_test)\n","\n","    # Calculate accuracy metrics\n","    accuracy = accuracy_score(y_test, predictions)\n","    f1 = f1_score(y_test, predictions, average=None)[-1] # for UIS class only\n","    report = classification_report(y_test, predictions)\n","    confusion = confusion_matrix(y_test, predictions)\n","\n","    return accuracy, confusion, report, predictions, f1\n","\n","selected_features =[\n","    # 'HH', 'HH_asm', 'HH_contrast', 'HH_corr', 'HH_dent', 'HH_diss', 'HH_dvar', 'HH_ent', 'HH_idm', 'HH_imcorr1', 'HH_imcorr2', 'HH_inertia', 'HH_prom', 'HH_savg', 'HH_sent', 'HH_shade', 'HH_svar', 'HH_var',\n","    # 'HV', 'HV_asm', 'HV_contrast', 'HV_corr', 'HV_dent', 'HV_diss', 'HV_dvar', 'HV_ent', 'HV_idm', 'HV_imcorr1', 'HV_imcorr2', 'HV_inertia', 'HV_prom', 'HV_savg', 'HV_sent', 'HV_shade', 'HV_svar', 'HV_var',\n","    'NSAI1', 'NSAI2', 'SISAI',\n","    # 'VH', 'VH_asm', 'VH_contrast', 'VH_corr', 'VH_dent', 'VH_diss', 'VH_dvar', 'VH_ent', 'VH_idm', 'VH_imcorr1', 'VH_imcorr2', 'VH_inertia', 'VH_prom', 'VH_savg', 'VH_sent', 'VH_shade', 'VH_svar', 'VH_var',\n","    # 'VV', 'VV_asm', 'VV_contrast', 'VV_corr', 'VV_dent', 'VV_diss', 'VV_dvar', 'VV_ent', 'VV_idm', 'VV_imcorr1', 'VV_imcorr2', 'VV_inertia', 'VV_prom', 'VV_savg', 'VV_sent', 'VV_shade', 'VV_svar', 'VV_var',\n","    'indbiMax', 'indbiMedian', 'mndwiMax', 'mndwiMedian', 'mndwiSD', 'ndbiMedian', 'ndbiMin', 'ndbiSD', 'nduiMedian', 'nduiMin', 'nduiSD', 'ndviMax', 'ndviMedian', 'ndviSD',\n","    's2_aerosols', 's2_blue', 's2_green', 's2_nir', 's2_red', 's2_redEdge1', 's2_redEdge2', 's2_redEdge3', 's2_redEdge4', 's2_swir1', 's2_swir2', 's2_waterVapor', 'swiRedMedian',\n","    'viirs'\n","                   ]\n","\n","def main():\n","    # Load the CSV file into a DataFrame\n","    data = pd.read_csv('/content/drive/MyDrive/ELM/merged_CSV/merged_132K.csv').dropna()\n","    data = data.drop_duplicates(subset=['.geo'])\n","\n","    print('Dataset Features :',data.columns.tolist())\n","\n","    # Define sample sizes for each class seperately\n","    # sample_sizes = {        0: 3000,        1: 3000,        2: 3000,        3: 3000,        4: 6000        }\n","    sample_sizes = {\n","        0: 100,\n","        1: 100,\n","        2: 100,\n","        3: 100,\n","        4: 100        }\n","\n","    grouped = data.groupby('Class')# Group by class\n","    sampled = grouped.apply(lambda x: x.sample(n=sample_sizes[x.name], replace=True).reset_index(drop=True))  # Sample from each group according to the defined sample sizes\n","    sampled = [sampled] # Convert to list\n","    data = pd.concat(sampled) # Concatenate samples\n","\n","    target = data['Class']\n","    data = data[selected_features]\n","\n","    print('Selected Features :',data.columns.tolist())\n","    print('Total sample numbers in each classes :')\n","    print(target.value_counts())\n","    print('n_estimators=250')\n","    print()\n","\n","    # Assuming 'data' is your DataFrame and 'target' is the target variable\n","    features = data.columns.tolist()\n","\n","    # Record the best accuracy and feature combination\n","    best_accuracy = 0\n","    best_combination = []\n","\n","    # Create a dictionary to store the accuracy of each feature\n","    feature_accuracies = {}\n","\n","    # Loop until no new feature is added to the best combination\n","    while True:\n","        best_feature = None\n","\n","        for feature in features:\n","            # Add the current feature to the best combination\n","            combination = best_combination + [feature]\n","\n","            # Split the data into training and test sets (with a random seed)\n","            X_train, X_test, y_train, y_test = train_test_split(data[combination], target, test_size=0.20, random_state=0)\n","\n","            # Train the model and evaluate it\n","            accuracy = train_and_evaluate_classifier(X_train, X_test, y_train, y_test)[4] # selected 4 for F1_score\n","\n","            # Store the accuracy of the feature\n","            feature_accuracies[feature] = accuracy\n","\n","            # If this combination has the best accuracy so far, record it\n","            if accuracy > best_accuracy:    # for regressor <, for classifiers >\n","                best_accuracy = accuracy\n","                best_feature = feature\n","                best_accuracy = accuracy\n","                best_X_train = X_train\n","                best_X_test = X_test\n","                best_y_train = y_train\n","                best_y_test = y_test\n","        # If no new feature was added to the best combination, stop the loop\n","        if best_feature is None:\n","            break\n","\n","        # Sort the features by their accuracies in ascending order and print them\n","        sorted_features = sorted(feature_accuracies.items(), key=lambda item: item[1], reverse=False)\n","\n","        # If we're evaluating a single feature, print its accuracy\n","        if len(combination) == 1:\n","            for feature, accuracy in sorted_features:\n","                print(f\"Accuracy: {accuracy:.4f}, Feature: {feature}\")\n","\n","        # Otherwise, add the best feature to the best combination\n","        best_combination.append(best_feature)\n","        features.remove(best_feature)\n","\n","        print(f\"Accuracy: {best_accuracy:.4f}, Combination: {best_combination}\")\n","\n","    print(f\"Best accuracy: {best_accuracy:.4f}, Best feature combination: {best_combination}\")\n","\n","    # Call function train_and_evaluate_regressor. Mainly to define the variable predictions\n","    accuracy, confusion, report, predictions, f1 = train_and_evaluate_classifier(best_X_train, best_X_test, best_y_train, best_y_test)\n","\n","    # use the prediction variable to  get accuracy metrics\n","    accuracy_metrics = evaluate_accuracy(best_y_test, predictions)\n","\n","if __name__ == \"__main__\":\n","   main()\n","\n","# Calculate and print the time it took to train the model\n","training_time = time.time() - start_time\n","print(f\"Training time: {training_time} seconds\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dJwexDs-t5oq","executionInfo":{"status":"ok","timestamp":1694567632185,"user_tz":-480,"elapsed":147548,"user":{"displayName":"Akib Javed","userId":"01752818298668219500"}},"outputId":"50214bb6-866f-4b2f-8570-068b72afcc6c"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset Features : ['system:index', 'City', 'Class', 'HH', 'HH_asm', 'HH_contrast', 'HH_corr', 'HH_dent', 'HH_diss', 'HH_dvar', 'HH_ent', 'HH_idm', 'HH_imcorr1', 'HH_imcorr2', 'HH_inertia', 'HH_maxcorr', 'HH_prom', 'HH_savg', 'HH_sent', 'HH_shade', 'HH_svar', 'HH_var', 'HV', 'HV_asm', 'HV_contrast', 'HV_corr', 'HV_dent', 'HV_diss', 'HV_dvar', 'HV_ent', 'HV_idm', 'HV_imcorr1', 'HV_imcorr2', 'HV_inertia', 'HV_maxcorr', 'HV_prom', 'HV_savg', 'HV_sent', 'HV_shade', 'HV_svar', 'HV_var', 'NSAI1', 'NSAI2', 'SISAI', 'VH', 'VH_asm', 'VH_contrast', 'VH_corr', 'VH_dent', 'VH_diss', 'VH_dvar', 'VH_ent', 'VH_idm', 'VH_imcorr1', 'VH_imcorr2', 'VH_inertia', 'VH_maxcorr', 'VH_prom', 'VH_savg', 'VH_sent', 'VH_shade', 'VH_svar', 'VH_var', 'VV', 'VV_asm', 'VV_contrast', 'VV_corr', 'VV_dent', 'VV_diss', 'VV_dvar', 'VV_ent', 'VV_idm', 'VV_imcorr1', 'VV_imcorr2', 'VV_inertia', 'VV_maxcorr', 'VV_prom', 'VV_savg', 'VV_sent', 'VV_shade', 'VV_svar', 'VV_var', 'indbiMax', 'indbiMedian', 'mndwiMax', 'mndwiMedian', 'mndwiSD', 'ndbiMedian', 'ndbiMin', 'ndbiSD', 'nduiMedian', 'nduiMin', 'nduiSD', 'ndviMax', 'ndviMedian', 'ndviSD', 's2_aerosols', 's2_blue', 's2_green', 's2_nir', 's2_red', 's2_redEdge1', 's2_redEdge2', 's2_redEdge3', 's2_redEdge4', 's2_swir1', 's2_swir2', 's2_waterVapor', 'swiRedMedian', 'viirs', '.geo']\n","Selected Features : ['NSAI1', 'NSAI2', 'SISAI', 'indbiMax', 'indbiMedian', 'mndwiMax', 'mndwiMedian', 'mndwiSD', 'ndbiMedian', 'ndbiMin', 'ndbiSD', 'nduiMedian', 'nduiMin', 'nduiSD', 'ndviMax', 'ndviMedian', 'ndviSD', 's2_aerosols', 's2_blue', 's2_green', 's2_nir', 's2_red', 's2_redEdge1', 's2_redEdge2', 's2_redEdge3', 's2_redEdge4', 's2_swir1', 's2_swir2', 's2_waterVapor', 'swiRedMedian', 'viirs']\n","Total sample numbers in each classes :\n","0.0    100\n","1.0    100\n","2.0    100\n","3.0    100\n","4.0    100\n","Name: Class, dtype: int64\n","n_estimators=250\n","\n","Accuracy: 0.1212, Feature: nduiMedian\n","Accuracy: 0.1429, Feature: mndwiSD\n","Accuracy: 0.1951, Feature: indbiMedian\n","Accuracy: 0.2162, Feature: s2_aerosols\n","Accuracy: 0.2222, Feature: ndbiMedian\n","Accuracy: 0.2222, Feature: swiRedMedian\n","Accuracy: 0.2286, Feature: ndbiMin\n","Accuracy: 0.2286, Feature: s2_blue\n","Accuracy: 0.2500, Feature: ndviMax\n","Accuracy: 0.2857, Feature: NSAI2\n","Accuracy: 0.2941, Feature: ndbiSD\n","Accuracy: 0.3478, Feature: s2_redEdge1\n","Accuracy: 0.3590, Feature: nduiMin\n","Accuracy: 0.3590, Feature: nduiSD\n","Accuracy: 0.3636, Feature: mndwiMax\n","Accuracy: 0.3750, Feature: s2_redEdge3\n","Accuracy: 0.3871, Feature: ndviMedian\n","Accuracy: 0.3889, Feature: s2_green\n","Accuracy: 0.4000, Feature: mndwiMedian\n","Accuracy: 0.4000, Feature: ndviSD\n","Accuracy: 0.4242, Feature: s2_swir2\n","Accuracy: 0.4324, Feature: NSAI1\n","Accuracy: 0.4865, Feature: s2_red\n","Accuracy: 0.4878, Feature: s2_redEdge2\n","Accuracy: 0.4889, Feature: s2_swir1\n","Accuracy: 0.5000, Feature: s2_nir\n","Accuracy: 0.5000, Feature: s2_waterVapor\n","Accuracy: 0.5366, Feature: s2_redEdge4\n","Accuracy: 0.5405, Feature: indbiMax\n","Accuracy: 0.5455, Feature: SISAI\n","Accuracy: 0.7568, Feature: viirs\n","Accuracy: 0.7568, Combination: ['viirs']\n","Accuracy: 0.9143, Combination: ['viirs', 's2_swir2']\n","Accuracy: 1.0000, Combination: ['viirs', 's2_swir2', 'ndbiMin']\n","Best accuracy: 1.0000, Best feature combination: ['viirs', 's2_swir2', 'ndbiMin']\n","\n","Accuracy Metrics:\n","\n","Accuracy: 0.890\n","Precision: 0.893\n","Recall: 0.895\n","F1 Score: 0.894\n","Kappa: 0.862\n","\n","Confusion Matrix:\n","[[16  1  0  0  0]\n"," [ 1 16  2  0  0]\n"," [ 1  1 17  2  0]\n"," [ 0  0  3 22  0]\n"," [ 0  0  0  0 18]]\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.89      0.94      0.91        17\n","         1.0       0.89      0.84      0.86        19\n","         2.0       0.77      0.81      0.79        21\n","         3.0       0.92      0.88      0.90        25\n","         4.0       1.00      1.00      1.00        18\n","\n","    accuracy                           0.89       100\n","   macro avg       0.89      0.89      0.89       100\n","weighted avg       0.89      0.89      0.89       100\n","\n","Training time: 147.14403438568115 seconds\n"]}]},{"cell_type":"code","source":["# REGRESSOR based  Best Feature Combination\n","# Version 14.4.1 (11 September 2023)\n","# add addtional accuracy matrix for best feature combination model\n","\n","# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestRegressor\n","from xgboost import XGBRegressor\n","from sklearn.ensemble import VotingRegressor\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","\n","# Define a function for Accuracy Assessment for regressor ML model\n","def evaluate_accuracy_regressor(y_true, y_pred):\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    mae = mean_absolute_error(y_true, y_pred)\n","    r2 = r2_score(y_true, y_pred)\n","\n","    print(\"\\nRegression Metrics:\\n\")\n","    print(f\"Mean Squared Error (MSE): {mse:.3f}\")\n","    print(f\"Root Mean Squared Error (RMSE): {rmse:.3f}\")\n","    print(f\"Mean Absolute Error (MAE): {mae:.3f}\")\n","    print(f\"R^2 Score: {r2:.3f}\")\n","\n","    return mse, rmse, mae, r2\n","\n","# Define a function for REGRESSION based model training and evaluation\n","def train_and_evaluate_regressor(X_train, X_test, y_train, y_test):\n","    # Define the regression models\n","    rf_model = RandomForestRegressor(n_estimators=250, random_state=100, n_jobs=-1)\n","    xgb_model = XGBRegressor(n_estimators=250, random_state=100, n_jobs=-1)\n","\n","    # Create a VotingRegressor with soft voting\n","    ensemble_model = VotingRegressor(\n","        estimators=[('rf', rf_model), ('xgb', xgb_model)]\n","    )\n","\n","    # Train the VotingRegressor\n","    ensemble_model.fit(X_train, y_train)\n","\n","    # Make predictions\n","    predictions = ensemble_model.predict(X_test)\n","\n","    # Calculate regression metrics\n","    mse = mean_squared_error(y_test, predictions)\n","    rmse = np.sqrt(mse)\n","    mae = mean_absolute_error(y_test, predictions)\n","    r2 = r2_score(y_test, predictions)\n","\n","    return mse, rmse, mae, r2, predictions\n","\n","selected_features =[\n","    'HH', 'HH_asm', 'HH_contrast', 'HH_corr', 'HH_dent', 'HH_diss', 'HH_dvar', 'HH_ent', 'HH_idm', 'HH_imcorr1', 'HH_imcorr2', 'HH_inertia', 'HH_prom', 'HH_savg', 'HH_sent', 'HH_shade', 'HH_svar', 'HH_var',\n","    'HV', 'HV_asm', 'HV_contrast', 'HV_corr', 'HV_dent', 'HV_diss', 'HV_dvar', 'HV_ent', 'HV_idm', 'HV_imcorr1', 'HV_imcorr2', 'HV_inertia', 'HV_prom', 'HV_savg', 'HV_sent', 'HV_shade', 'HV_svar', 'HV_var',\n","    'NSAI1', 'NSAI2', 'SISAI',\n","    'VH', 'VH_asm', 'VH_contrast', 'VH_corr', 'VH_dent', 'VH_diss', 'VH_dvar', 'VH_ent', 'VH_idm', 'VH_imcorr1', 'VH_imcorr2', 'VH_inertia', 'VH_prom', 'VH_savg', 'VH_sent', 'VH_shade', 'VH_svar', 'VH_var',\n","    'VV', 'VV_asm', 'VV_contrast', 'VV_corr', 'VV_dent', 'VV_diss', 'VV_dvar', 'VV_ent', 'VV_idm', 'VV_imcorr1', 'VV_imcorr2', 'VV_inertia', 'VV_prom', 'VV_savg', 'VV_sent', 'VV_shade', 'VV_svar', 'VV_var',\n","    'indbiMax', 'indbiMedian', 'mndwiMax', 'mndwiMedian', 'mndwiSD', 'ndbiMedian', 'ndbiMin', 'ndbiSD', 'nduiMedian', 'nduiMin', 'nduiSD', 'ndviMax', 'ndviMedian', 'ndviSD',\n","    's2_aerosols', 's2_blue', 's2_green', 's2_nir', 's2_red', 's2_redEdge1', 's2_redEdge2', 's2_redEdge3', 's2_redEdge4', 's2_swir1', 's2_swir2', 's2_waterVapor', 'swiRedMedian',\n","    'viirs'\n","                   ]\n","\n","\n","\n","def main():\n","    # Load the CSV file into a DataFrame\n","    data = pd.read_csv('/content/drive/MyDrive/ELM/merged_CSV/merged_132K.csv').dropna()\n","    data = data.drop_duplicates(subset=['.geo'])\n","\n","    print('Dataset Features :',data.columns.tolist())\n","\n","    # Define sample sizes for each class seperately\n","    # sample_sizes = {        0: 3000,        1: 3000,        2: 3000,        3: 3000,        4: 6000        }\n","    sample_sizes = {        0: 150,        1: 150,        2: 150,        3: 150,        4: 150        }\n","\n","    grouped = data.groupby('Class')# Group by class\n","    sampled = grouped.apply(lambda x: x.sample(n=sample_sizes[x.name], replace=True).reset_index(drop=True))  # Sample from each group according to the defined sample sizes\n","    sampled = [sampled] # Convert to list\n","    data = pd.concat(sampled) # Concatenate samples\n","\n","    target = data['Class']\n","    y = target\n","    data = data[selected_features]\n","\n","    print('Selected Features :',data.columns.tolist())\n","    print('Total sample numbers in each classes :')\n","    print(target.value_counts())\n","    print('n_estimators=250')\n","    print()\n","\n","    # Assuming 'data' is your DataFrame and 'target' is the target variable\n","    features = data.columns.tolist()\n","\n","    # Record the best accuracy and feature combination\n","    best_accuracy = 0\n","    best_combination = []\n","\n","    # Create a dictionary to store the accuracy of each feature\n","    feature_accuracies = {}\n","\n","    best_X_train, best_X_test, best_y_train, best_y_test = None, None, None, None\n","\n","    # Loop until no new feature is added to the best combination\n","    while True:\n","        best_feature = None\n","\n","        for feature in features:\n","            # Add the current feature to the best combination\n","            combination = best_combination + [feature]\n","\n","            # Split the data into training and test sets (with a random seed)\n","            X_train, X_test, y_train, y_test = train_test_split(data[combination], target, test_size=0.20, random_state=0)\n","\n","            # Train the model and evaluate it\n","            accuracy = train_and_evaluate_regressor(X_train, X_test, y_train, y_test)[1] # selected 1 for RMSE\n","\n","            # Store the accuracy of the feature\n","            feature_accuracies[feature] = accuracy\n","\n","            # If this combination has the best accuracy so far, record it\n","            if accuracy < best_accuracy:    # for regressor <, for classifiers >\n","                best_accuracy = accuracy\n","                best_feature = feature\n","                best_accuracy = accuracy\n","                best_X_train = X_train\n","                best_X_test = X_test\n","                best_y_train = y_train\n","                best_y_test = y_test\n","        # If no new feature was added to the best combination, stop the loop\n","        if best_feature is None:\n","            break\n","\n","        # Sort the features by their accuracies in ascending order and print them\n","        sorted_features = sorted(feature_accuracies.items(), key=lambda item: item[1], reverse=True)\n","\n","        # If we're evaluating a single feature, print its accuracy\n","        if len(combination) == 1:\n","            for feature, accuracy in sorted_features:\n","                print(f\"Accuracy: {accuracy:.4f}, Feature: {feature}\")\n","\n","        # Otherwise, add the best feature to the best combination\n","        best_combination.append(best_feature)\n","        features.remove(best_feature)\n","\n","        print(f\"Accuracy: {best_accuracy:.4f}, Combination: {best_combination}\")\n","\n","    print(f\"Best accuracy: {best_accuracy:.4f}, Best feature combination: {best_combination}\")\n","\n","    # Call function train_and_evaluate_regressor. Mainly to define the variable predictions\n","    # accuracy, confusion, report, predictions, f1 = train_and_evaluate_classifier(best_X_train, best_X_test, best_y_train, best_y_test)\n","    # Train model with new best feature combo\n","    mse, rmse, mae, r2, predictions = train_and_evaluate_regressor(\n","        best_X_train, best_X_test, best_y_train, best_y_test\n","    )\n","    # use the prediction variable to  get accuracy metrics\n","    accuracy_metrics = evaluate_accuracy_regressor(best_y_test, predictions)\n","\n","if __name__ == \"__main__\":\n","   main()\n","# Code for testing best Regressor based ML features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":966},"id":"My0RRYAMXm8Z","executionInfo":{"status":"error","timestamp":1694541964321,"user_tz":-480,"elapsed":96675,"user":{"displayName":"Akib Javed","userId":"01752818298668219500"}},"outputId":"ab9e33a8-a20a-4ed9-a8a3-ce2606a52c45"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset Features : ['system:index', 'City', 'Class', 'HH', 'HH_asm', 'HH_contrast', 'HH_corr', 'HH_dent', 'HH_diss', 'HH_dvar', 'HH_ent', 'HH_idm', 'HH_imcorr1', 'HH_imcorr2', 'HH_inertia', 'HH_maxcorr', 'HH_prom', 'HH_savg', 'HH_sent', 'HH_shade', 'HH_svar', 'HH_var', 'HV', 'HV_asm', 'HV_contrast', 'HV_corr', 'HV_dent', 'HV_diss', 'HV_dvar', 'HV_ent', 'HV_idm', 'HV_imcorr1', 'HV_imcorr2', 'HV_inertia', 'HV_maxcorr', 'HV_prom', 'HV_savg', 'HV_sent', 'HV_shade', 'HV_svar', 'HV_var', 'NSAI1', 'NSAI2', 'SISAI', 'VH', 'VH_asm', 'VH_contrast', 'VH_corr', 'VH_dent', 'VH_diss', 'VH_dvar', 'VH_ent', 'VH_idm', 'VH_imcorr1', 'VH_imcorr2', 'VH_inertia', 'VH_maxcorr', 'VH_prom', 'VH_savg', 'VH_sent', 'VH_shade', 'VH_svar', 'VH_var', 'VV', 'VV_asm', 'VV_contrast', 'VV_corr', 'VV_dent', 'VV_diss', 'VV_dvar', 'VV_ent', 'VV_idm', 'VV_imcorr1', 'VV_imcorr2', 'VV_inertia', 'VV_maxcorr', 'VV_prom', 'VV_savg', 'VV_sent', 'VV_shade', 'VV_svar', 'VV_var', 'indbiMax', 'indbiMedian', 'mndwiMax', 'mndwiMedian', 'mndwiSD', 'ndbiMedian', 'ndbiMin', 'ndbiSD', 'nduiMedian', 'nduiMin', 'nduiSD', 'ndviMax', 'ndviMedian', 'ndviSD', 's2_aerosols', 's2_blue', 's2_green', 's2_nir', 's2_red', 's2_redEdge1', 's2_redEdge2', 's2_redEdge3', 's2_redEdge4', 's2_swir1', 's2_swir2', 's2_waterVapor', 'swiRedMedian', 'viirs', '.geo']\n","Selected Features : ['HH', 'HH_asm', 'HH_contrast', 'HH_corr', 'HH_dent', 'HH_diss', 'HH_dvar', 'HH_ent', 'HH_idm', 'HH_imcorr1', 'HH_imcorr2', 'HH_inertia', 'HH_prom', 'HH_savg', 'HH_sent', 'HH_shade', 'HH_svar', 'HH_var', 'HV', 'HV_asm', 'HV_contrast', 'HV_corr', 'HV_dent', 'HV_diss', 'HV_dvar', 'HV_ent', 'HV_idm', 'HV_imcorr1', 'HV_imcorr2', 'HV_inertia', 'HV_prom', 'HV_savg', 'HV_sent', 'HV_shade', 'HV_svar', 'HV_var', 'NSAI1', 'NSAI2', 'SISAI', 'VH', 'VH_asm', 'VH_contrast', 'VH_corr', 'VH_dent', 'VH_diss', 'VH_dvar', 'VH_ent', 'VH_idm', 'VH_imcorr1', 'VH_imcorr2', 'VH_inertia', 'VH_prom', 'VH_savg', 'VH_sent', 'VH_shade', 'VH_svar', 'VH_var', 'VV', 'VV_asm', 'VV_contrast', 'VV_corr', 'VV_dent', 'VV_diss', 'VV_dvar', 'VV_ent', 'VV_idm', 'VV_imcorr1', 'VV_imcorr2', 'VV_inertia', 'VV_prom', 'VV_savg', 'VV_sent', 'VV_shade', 'VV_svar', 'VV_var', 'indbiMax', 'indbiMedian', 'mndwiMax', 'mndwiMedian', 'mndwiSD', 'ndbiMedian', 'ndbiMin', 'ndbiSD', 'nduiMedian', 'nduiMin', 'nduiSD', 'ndviMax', 'ndviMedian', 'ndviSD', 's2_aerosols', 's2_blue', 's2_green', 's2_nir', 's2_red', 's2_redEdge1', 's2_redEdge2', 's2_redEdge3', 's2_redEdge4', 's2_swir1', 's2_swir2', 's2_waterVapor', 'swiRedMedian', 'viirs']\n","Total sample numbers in each classes :\n","0.0    150\n","1.0    150\n","2.0    150\n","3.0    150\n","4.0    150\n","Name: Class, dtype: int64\n","n_estimators=250\n","\n","Best accuracy: 0.0000, Best feature combination: []\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-d039da127495>\u001b[0m in \u001b[0;36m<cell line: 163>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m    \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;31m# Code for testing best Regressor based ML features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-d039da127495>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;31m# accuracy, confusion, report, predictions, f1 = train_and_evaluate_classifier(best_X_train, best_X_test, best_y_train, best_y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;31m# Train model with new best feature combo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     mse, rmse, mae, r2, predictions = train_and_evaluate_regressor(\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mbest_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_X_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_y_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     ) \n","\u001b[0;32m<ipython-input-16-d039da127495>\u001b[0m in \u001b[0;36mtrain_and_evaluate_regressor\u001b[0;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Train the VotingRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mensemble_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \"\"\"\n\u001b[1;32m    596\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, dtype, warn)\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m   1203\u001b[0m         \u001b[0;34m\"y should be a 1d array, got an array of shape {} instead.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape () instead."]}]},{"cell_type":"code","source":["# Building soft ensemble model with the best feature combination\n","# use after Version 14.3 of Best feature Combination findings\n","# 5 Class ML model training\n","# Version 7 (September 13, 2023)\n","\n","# Import libraries\n","import pandas as pd\n","import numpy as np\n","\n","import xgboost as xgb\n","from xgboost import XGBClassifier\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.metrics import (\n","    accuracy_score,\n","    precision_score,\n","    recall_score,\n","    f1_score,\n","    confusion_matrix,\n","    classification_report,\n","    cohen_kappa_score\n",")\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import joblib\n","\n","\n","# Define model evaluation function for classifer ML model\n","def evaluate_classifier_model(y_true, y_pred):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average=None)[-1]\n","    recall = recall_score(y_true, y_pred, average=None)[-1]\n","    f1 = f1_score(y_true, y_pred, average=None)[-1]\n","    confusion = confusion_matrix(y_true, y_pred)\n","    report = classification_report(y_true, y_pred)\n","    kappa = cohen_kappa_score(y_true, y_pred)\n","\n","    print(\"Accuracy: {:.3f}\".format(accuracy))\n","    print(\"Precision: {:.3f}\".format(precision))\n","    print(\"Recall: {:.3f}\".format(recall))\n","    print(\"F1 Score: {:.3f}\".format(f1))\n","    print(\"Kappa: {:.3f}\".format(kappa))\n","    print(\"\\nConfusion Matrix:\")\n","    print(confusion)\n","    print(\"\\nClassification Report:\")\n","    print(report)\n","\n","    return accuracy, precision, recall, f1, kappa, confusion, report\n","\n","# Define model evaluation function for regressor ML model\n","def evaluate_regressor_model(y_true, y_pred):\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    mae = mean_absolute_error(y_true, y_pred)\n","    r2 = r2_score(y_true, y_pred)\n","    predictions = ensemble_model.predict(X_test)\n","\n","    print(\"Mean Squared Error (MSE): {:.3f}\".format(mse))\n","    print(\"Root Mean Squared Error (RMSE): {:.3f}\".format(rmse))\n","    print(\"Mean Absolute Error (MAE): {:.3f}\".format(mae))\n","    print(\"R^2 Score: {:.3f}\".format(r2))\n","    print(\"Predictions: {:.3f}\".format(predictions))\n","\n","    return mse, rmse, mae, r2, predictions\n","\n","# Load data\n","# # Load data and drop any rows contain NaN values\n","data = pd.read_csv('/content/drive/MyDrive/ELM/merged_CSV/merged_132K.csv').dropna()\n","data = data.drop_duplicates(subset=['.geo'])\n","# data = merged_df.dropna()\n","\n","# # Define sample sizes\n","# sample_sizes = {\n","#     0: 10000,\n","#     1: 10000,\n","#     2: 10000,\n","#     3: 10000,\n","#     4: 10000\n","#     }\n","    # Define sample sizes\n","sample_sizes = {\n","    0: 1000,\n","    1: 1000,\n","    2: 1000,\n","    3: 1000,\n","    4: 1000\n","    }\n","\n","grouped = data.groupby('Class')# Group by class\n","sampled = grouped.apply(lambda x: x.sample(n=sample_sizes[x.name], replace=True).reset_index(drop=True))  # Sample from each group according to the defined sample sizes\n","sampled = [sampled] # Convert to list\n","sampled = pd.concat(sampled) # Concatenate samples\n","\n","# Verify counts\n","print('Total sample numbers in each classes :')\n","print(sampled['Class'].value_counts())\n","print()\n","\n","SFM = ['SISAI']\n","MFM = ['swirSoil', 'CI', 'Green', 'NIR', 'DBSI', 'NSAI2', 'Blue']\n","\n","models = [SFM, MFM]\n","model_name = ['SFM', 'MFM']\n","\n","\n","# Start of the loop function\n","for i in range(len(models)):\n","    # Split data\n","    X = sampled[models[i]]\n","    y = sampled['Class']\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n","\n","    # # Train a Classifier models\n","    # rf = RandomForestClassifier(n_estimators=500, random_state=100).fit(X_train, y_train)\n","    # xgb = xgb.XGBClassifier(n_estimators=500, random_state=100).fit(X_train, y_train)\n","    # ensemble_model = VotingClassifier(estimators=[('Random Forest', rf), ('XGBoost', xgb)], voting='soft').fit(X_train, y_train)\n","\n","    # Train a Regressor Model\n","    rf = RandomForestRegressor(n_estimators=500, random_state=100).fit(X_train, y_train)\n","    xgb = xgb.XGBRegressor(n_estimators=500, random_state=100).fit(X_train, y_train)\n","    ensemble_model = VotingRegressor(estimators=[('Random Forest', rf), ('XGBoost', xgb)]).fit(X_train, y_train)\n","\n","    # # Save model\n","    # joblib.dump(ensemble_model, f'/content/drive/MyDrive/ELM/TrainedModel/SEM_UIS_{models[i]}_230911.pkl')\n","\n","    # Evaluate model\n","    ensemble_pred = ensemble_model.predict(X_test)\n","    ensemble_accuracy = accuracy_score(y_test, ensemble_pred)\n","\n","    conf_mat_rf = confusion_matrix(y_test, rf_pred)  # confusion matrix for RF Classifier\n","    conf_mat_xgb = confusion_matrix(y_test, xgb_pred)  # confusion matrix for XGB Classifier\n","    conf_mat_ensemble = confusion_matrix(y_test, ensemble_pred)  # confusion matrix for Soft Ensemble Classifier\n","\n","    # Additional metrics\n","    ensemble_precision = precision_score(y_test, ensemble_pred, average=None)\n","    ensemble_recall = recall_score(y_test, ensemble_pred, average=None)\n","    ensemble_f1 = f1_score(y_test, ensemble_pred, average=None)\n","\n","    print('Model :', i+1)\n","    print('dataset shape :', X.shape)\n","    print('X.columns.tolist() :',X.columns.tolist())\n","\n","    # Make predictions\n","    y_pred = ensemble_pred\n","\n","    # Evaluate model\n","    evaluate_model(y_test, y_pred)\n","\n","\n","\n","  # # print('confusion matrix :')\n","  # print(conf_mat_ensemble)\n","  # print()\n","\n","  # # Print ensemble metrics\n","  # # print('Ensemble Test Metrics:')\n","  # print(i+1, ':','Ensemble_OA: ', ensemble_accuracy)\n","\n","  # # Precision, Recall and F1_Score is for Impervious class only.\n","  # print(i+1, ':','Precision: ', ensemble_precision[3])\n","  # print(i+1, ':','Recall: ', ensemble_recall[3])\n","  # print(i+1, ':','F1_Score: ', ensemble_f1[3])\n","  # # print()\n"],"metadata":{"id":"kLDDk4StIRB6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Building soft ensemble model with the best feature combination\n","# use after Version 14.2 of Best feature Combination findings\n","# 5 Class ML model training\n","# Version 6 (September 09, 2023)\n","\n","# Import libraries\n","import pandas as pd\n","import numpy as np\n","\n","import xgboost as xgb\n","from xgboost import XGBClassifier\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.metrics import (\n","    accuracy_score,\n","    precision_score,\n","    recall_score,\n","    f1_score,\n","    confusion_matrix,\n","    classification_report,\n","    cohen_kappa_score\n",")\n","import joblib\n","\n","\n","# Define model evaluation function\n","def evaluate_model(y_true, y_pred):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average=None)[3]\n","    recall = recall_score(y_true, y_pred, average=None)[3]\n","    f1 = f1_score(y_true, y_pred, average=None)[3]\n","    confusion = confusion_matrix(y_true, y_pred)\n","    report = classification_report(y_true, y_pred)\n","    kappa = cohen_kappa_score(y_true, y_pred)\n","\n","    print(\"Accuracy: {:.3f}\".format(accuracy))\n","    print(\"Precision: {:.3f}\".format(precision))\n","    print(\"Recall: {:.3f}\".format(recall))\n","    print(\"F1 Score: {:.3f}\".format(f1))\n","    print(\"Kappa: {:.3f}\".format(kappa))\n","\n","    print(\"\\nConfusion Matrix:\")\n","    print(confusion)\n","\n","    print(\"\\nClassification Report:\")\n","    print(report)\n","\n","    return accuracy, precision, recall, f1, kappa, confusion, report\n","\n","# Load data\n","# # Load data and drop any rows contain NaN values\n","data = pd.read_csv('/content/drive/MyDrive/ELM/merged_CSV/merged_60K_Soil_Five_Classes.csv').dropna()\n","data = data.drop_duplicates(subset=['.geo'])\n","# data = merged_df.dropna()\n","\n","# Define sample sizes\n","sample_sizes = {\n","    0: 10000,\n","    1: 10000,\n","    2: 10000,\n","    3: 10000,\n","    4: 10000\n","    }\n","\n","grouped = data.groupby('Class')# Group by class\n","sampled = grouped.apply(lambda x: x.sample(n=sample_sizes[x.name], replace=True).reset_index(drop=True))  # Sample from each group according to the defined sample sizes\n","sampled = [sampled] # Convert to list\n","sampled = pd.concat(sampled) # Concatenate samples\n","\n","# Verify counts\n","print('Total sample numbers in each classes :')\n","print(sampled['Class'].value_counts())\n","print()\n","\n","SFM = ['SISAI']\n","MFM = ['swirSoil', 'CI', 'Green', 'NIR', 'DBSI', 'NSAI2', 'Blue']\n","\n","models = [SFM, MFM]\n","model_name = ['SFM', 'MFM']\n","\n","\n","# Train models\n","# Start of the loop function\n","for i in range(len(models)):\n","\n","  # Split the data into features (X) and labels (y)\n","  # X = data.drop('Class', axis=1)\n","  X = sampled[models[i]] # , 'Class'\n","  y = sampled['Class']\n","\n","  # Split data\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n","\n","  # Final Random Forest Model\n","  rf_final = RandomForestClassifier(n_estimators=500, random_state=100)\n","  rf_final.fit(X_train, y_train)\n","  rf_pred = rf_final.predict(X_test)\n","  rf_accuracy = accuracy_score(y_test, rf_pred)\n","\n","  # Final XGB Model\n","  xgb_final = xgb.XGBClassifier(n_estimators=500, random_state=100)\n","  xgb_final.fit(X_train, y_train)\n","  xgb_pred = xgb_final.predict(X_test)\n","  xgb_accuracy = accuracy_score(y_test, xgb_pred)\n","\n","  # Create a VotingClassifier with all the individual classifiers\n","  ensemble_model = VotingClassifier(\n","      estimators=[\n","          ('Random Forest', rf_final),\n","          ('XGBoost', xgb_final)\n","                  ],\n","                  voting='soft'\n","  )\n","\n","  # Fit model with data\n","  ensemble_model.fit(X_train, y_train)\n","\n","  # Save the trained Ensembled model to disk\n","  # SEM stands for Soft Ensemble Model\n","  joblib.dump(ensemble_model, f'/content/drive/MyDrive/ELM/TrainedModel/SEM_Soil_{model_name[i]}_230911.pkl')\n","\n","  # Ensemble predictions & Accuracy\n","  ensemble_pred = ensemble_model.predict(X_test)\n","  ensemble_accuracy = accuracy_score(y_test, ensemble_pred)\n","\n","  conf_mat_rf = confusion_matrix(y_test, rf_pred)  # confusion matrix for RF Classifier\n","  conf_mat_xgb = confusion_matrix(y_test, xgb_pred)  # confusion matrix for XGB Classifier\n","  conf_mat_ensemble = confusion_matrix(y_test, ensemble_pred)  # confusion matrix for Soft Ensemble Classifier\n","\n","  # Additional metrics\n","  ensemble_precision = precision_score(y_test, ensemble_pred, average=None)\n","  ensemble_recall = recall_score(y_test, ensemble_pred, average=None)\n","  ensemble_f1 = f1_score(y_test, ensemble_pred, average=None)\n","\n","  print('Model :', i+1)\n","  print('dataset shape :', X.shape)\n","  print('X.columns.tolist() :',X.columns.tolist())\n","\n","  # Make predictions\n","  y_pred = ensemble_pred\n","\n","  # Evaluate model\n","  evaluate_model(y_test, y_pred)\n","\n","\n","\n","  # # print('confusion matrix :')\n","  # print(conf_mat_ensemble)\n","  # print()\n","\n","  # # Print ensemble metrics\n","  # # print('Ensemble Test Metrics:')\n","  # print(i+1, ':','Ensemble_OA: ', ensemble_accuracy)\n","\n","  # # Precision, Recall and F1_Score is for Impervious class only.\n","  # print(i+1, ':','Precision: ', ensemble_precision[3])\n","  # print(i+1, ':','Recall: ', ensemble_recall[3])\n","  # print(i+1, ':','F1_Score: ', ensemble_f1[3])\n","  # # print()\n"],"metadata":{"id":"ZVy7nL1bXSZM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Finding Best Feature Combination for building ML Model\n","# Version 14 (11 September 2023)\n","# removing pipeline transformer and Scaler as these are unnecessary for the classifier's we are using.\n","\n","# Import necessary libraries\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","import xgboost as xgb\n","\n","# Define a function for model training and evaluation\n","def train_and_evaluate(X_train, X_test, y_train, y_test):\n","    # Define the classifiers\n","    rf_model = RandomForestClassifier(n_estimators=250, random_state=100, n_jobs=-1)\n","    xgb_model = xgb.XGBClassifier(n_estimators=250, random_state=100, n_jobs=-1)\n","\n","    # Create a VotingClassifier with soft voting\n","    ensemble_model = VotingClassifier(\n","        estimators=[('rf', rf_model), ('xgb', xgb_model)],\n","        voting='soft'\n","    )\n","\n","    # Train the VotingClassifier\n","    ensemble_model.fit(X_train, y_train)\n","\n","    # Make predictions and calculate accuracy\n","    predictions = ensemble_model.predict(X_test)\n","    accuracy = f1_score(y_test, predictions, average=None)[3] #3 is for soil class\n","\n","    return accuracy\n","\n","def main():\n","    # Load the CSV file into a DataFrame\n","    data = pd.read_csv('/content/drive/MyDrive/ELM/merged_CSV/merged_60K_Soil_Five_Classes.csv').dropna()\n","    data = data.drop_duplicates(subset=['.geo'])\n","\n","    print('data.columns.tolist() :',data.columns.tolist())\n","\n","    # Define sample sizes for each class seperately\n","    sample_sizes = {\n","        0: 10000,\n","        1: 10000,\n","        2: 10000,\n","        3: 10000,\n","        4: 10000\n","        }\n","\n","    grouped = data.groupby('Class')# Group by class\n","    sampled = grouped.apply(lambda x: x.sample(n=sample_sizes[x.name], replace=True).reset_index(drop=True))  # Sample from each group according to the defined sample sizes\n","    sampled = [sampled] # Convert to list\n","    data = pd.concat(sampled) # Concatenate samples\n","\n","    target = data['Class']\n","    data = data.drop(['Class','.geo','system:index', 'City'], axis=1)\n","\n","    # Verify counts\n","    print('Total sample numbers in each classes :')\n","    print(target.value_counts())\n","    print('n_estimators=250')\n","    print()\n","\n","    # Assuming 'data' is your DataFrame and 'target' is the target variable\n","    features = data.columns.tolist()\n","\n","    # Record the best accuracy and feature combination\n","    best_accuracy = 0\n","    best_combination = []\n","\n","    # Create a dictionary to store the accuracy of each feature\n","    feature_accuracies = {}\n","\n","    # Loop until no new feature is added to the best combination\n","    while True:\n","        best_feature = None\n","\n","        for feature in features:\n","            # Add the current feature to the best combination\n","            combination = best_combination + [feature]\n","\n","            # Split the data into training and test sets (with a random seed)\n","            X_train, X_test, y_train, y_test = train_test_split(data[combination], target, test_size=0.20, random_state=0)\n","\n","            # Train the model and evaluate it\n","            accuracy = train_and_evaluate(X_train, X_test, y_train, y_test)\n","\n","            # Store the accuracy of the feature\n","            feature_accuracies[feature] = accuracy\n","\n","            # If this combination has the best accuracy so far, record it\n","            if accuracy > best_accuracy:\n","                best_accuracy = accuracy\n","                best_feature = feature\n","\n","        # If no new feature was added to the best combination, stop the loop\n","        if best_feature is None:\n","            break\n","\n","        # Sort the features by their accuracies in ascending order and print them\n","        sorted_features = sorted(feature_accuracies.items(), key=lambda item: item[1])\n","\n","        # If we're evaluating a single feature, print its accuracy\n","        if len(combination) == 1:\n","            for feature, accuracy in sorted_features:\n","                print(f\"Accuracy: {accuracy}, Feature: {feature}\")\n","\n","        # Otherwise, add the best feature to the best combination\n","        best_combination.append(best_feature)\n","        features.remove(best_feature)\n","\n","        print(f\"Accuracy: {best_accuracy}\", f\"Combination: {best_combination}\")\n","\n","    print(f\"Best accuracy: {best_accuracy}\")\n","    print(f\"Best feature combination: {best_combination}\")\n","\n","if __name__ == \"__main__\":\n","   main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iUSHi7FWw6Tn","outputId":"89e10b3c-5a3f-4bce-def4-3eeef82e90a8","executionInfo":{"status":"ok","timestamp":1694402990328,"user_tz":-480,"elapsed":2160989,"user":{"displayName":"Akib Javed","userId":"01752818298668219500"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["data.columns.tolist() : ['system:index', 'Blue', 'CI', 'City', 'Class', 'DBSI', 'Green', 'NDSI2010', 'NDSI2015', 'NIR', 'NSAI1', 'NSAI2', 'RNDSI', 'Red', 'SWIR1', 'SWIR2', 'swirSoil', '.geo']\n","Total sample numbers in each classes :\n","0    10000\n","1    10000\n","2    10000\n","3    10000\n","4    10000\n","Name: Class, dtype: int64\n","n_estimators=250\n","\n","Accuracy: 0.7859281437125749, Feature: NIR\n","Accuracy: 0.792079207920792, Feature: NDSI2015\n","Accuracy: 0.7956452710365163, Feature: NSAI2\n","Accuracy: 0.7964847363552268, Feature: NSAI1\n","Accuracy: 0.8111888111888111, Feature: RNDSI\n","Accuracy: 0.814466885861907, Feature: CI\n","Accuracy: 0.8192346424974823, Feature: Blue\n","Accuracy: 0.8298792985652471, Feature: NDSI2010\n","Accuracy: 0.8408348001005782, Feature: Green\n","Accuracy: 0.8434393638170975, Feature: SWIR1\n","Accuracy: 0.8496672022033509, Feature: DBSI\n","Accuracy: 0.8562156764852721, Feature: SWIR2\n","Accuracy: 0.8618172665492071, Feature: Red\n","Accuracy: 0.9001956947162427, Feature: swirSoil\n","Accuracy: 0.9001956947162427 Combination: ['swirSoil']\n","Accuracy: 0.9317048853439681 Combination: ['swirSoil', 'CI']\n","Accuracy: 0.9478720725258122 Combination: ['swirSoil', 'CI', 'Green']\n","Accuracy: 0.9714285714285714 Combination: ['swirSoil', 'CI', 'Green', 'NIR']\n","Accuracy: 0.9741935483870968 Combination: ['swirSoil', 'CI', 'Green', 'NIR', 'DBSI']\n","Accuracy: 0.9756461232604374 Combination: ['swirSoil', 'CI', 'Green', 'NIR', 'DBSI', 'NSAI2']\n","Accuracy: 0.9763622791739238 Combination: ['swirSoil', 'CI', 'Green', 'NIR', 'DBSI', 'NSAI2', 'Blue']\n","Best accuracy: 0.9763622791739238\n","Best feature combination: ['swirSoil', 'CI', 'Green', 'NIR', 'DBSI', 'NSAI2', 'Blue']\n"]}]},{"cell_type":"code","source":["# Finding Best Feature Combination for building ML Model\n","# Version 13 (11 September 2023)\n","\n","# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","import xgboost as xgb\n","\n","# Define a function for model training and evaluation\n","def train_and_evaluate(X_train, X_test, y_train, y_test):\n","    # Create a pipeline for data preprocessing and model training\n","    pipeline = Pipeline([\n","        ('scaling', StandardScaler()),\n","        ('classification', VotingClassifier(\n","            estimators=[\n","                ('Random Forest', RandomForestClassifier(n_estimators=250, random_state=100, n_jobs=-1)),\n","                ('XGBoost', xgb.XGBClassifier(n_estimators=250, random_state=100, n_jobs=-1))\n","            ],\n","            voting='soft'\n","        ))\n","    ])\n","\n","    # Train the model using the pipeline\n","    pipeline.fit(X_train, y_train)\n","\n","    # Make predictions and calculate accuracy\n","    predictions = pipeline.predict(X_test)\n","    accuracy = f1_score(y_test, predictions, average=None)[3] #3 is for soil class\n","\n","    return accuracy\n","\n","def main():\n","    # Load the CSV file into a DataFrame\n","    data = pd.read_csv('/content/drive/MyDrive/ELM/merged_CSV/merged_60K_Soil_Five_Classes.csv').dropna()\n","    data = data.drop_duplicates(subset=['.geo'])\n","\n","    print('data.columns.tolist() :',data.columns.tolist())\n","\n","    # Define sample sizes\n","    sample_sizes = {\n","        0: 10000,\n","        1: 10000,\n","        2: 10000,\n","        3: 10000,\n","        4: 10000\n","        }\n","\n","    grouped = data.groupby('Class')# Group by class\n","    sampled = grouped.apply(lambda x: x.sample(n=sample_sizes[x.name], replace=True).reset_index(drop=True))  # Sample from each group according to the defined sample sizes\n","    sampled = [sampled] # Convert to list\n","    data = pd.concat(sampled) # Concatenate samples\n","\n","    target = data['Class']\n","    data = data.drop(['Class','.geo','system:index', 'City'], axis=1)\n","\n","    # Verify counts\n","    print('Total sample numbers in each classes :')\n","    print(target.value_counts())\n","    print('n_estimators=250')\n","    print()\n","\n","    # Assuming 'data' is your DataFrame and 'target' is the target variable\n","    features = data.columns.tolist()\n","\n","    # Record the best accuracy and feature combination\n","    best_accuracy = 0\n","    best_combination = []\n","\n","    # Create a dictionary to store the accuracy of each feature\n","    feature_accuracies = {}\n","\n","    # Loop until no new feature is added to the best combination\n","    while True:\n","        best_feature = None\n","\n","        for feature in features:\n","            # Add the current feature to the best combination\n","            combination = best_combination + [feature]\n","\n","            # Split the data into training and test sets (with a random seed)\n","            X_train, X_test, y_train, y_test = train_test_split(data[combination], target, test_size=0.20, random_state=0)\n","\n","            # Train the model and evaluate it\n","            accuracy = train_and_evaluate(X_train, X_test, y_train, y_test)\n","\n","            # Store the accuracy of the feature\n","            feature_accuracies[feature] = accuracy\n","\n","            # If this combination has the best accuracy so far, record it\n","            if accuracy > best_accuracy:\n","                best_accuracy = accuracy\n","                best_feature = feature\n","\n","        # If no new feature was added to the best combination, stop the loop\n","        if best_feature is None:\n","            break\n","\n","        # Sort the features by their accuracies in ascending order and print them\n","        sorted_features = sorted(feature_accuracies.items(), key=lambda item: item[1])\n","\n","        # If we're evaluating a single feature, print its accuracy\n","        if len(combination) == 1:\n","            for feature, accuracy in sorted_features:\n","                print(f\"Accuracy: {accuracy}, Feature: {feature}\")\n","\n","        # Otherwise, add the best feature to the best combination\n","        best_combination.append(best_feature)\n","        features.remove(best_feature)\n","\n","        print(f\"Accuracy: {best_accuracy}\", f\"Combination: {best_combination}\")\n","\n","    print(f\"Best accuracy: {best_accuracy}\")\n","    print(f\"Best feature combination: {best_combination}\")\n","\n","if __name__ == \"__main__\":\n","   main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":807},"id":"zzNpodCCCj8F","executionInfo":{"status":"error","timestamp":1694394992732,"user_tz":-480,"elapsed":1858032,"user":{"displayName":"Akib Javed","userId":"01752818298668219500"}},"outputId":"8a7c4dfb-e8c4-4f17-af14-c6cf7c23fed9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["data.columns.tolist() : ['system:index', 'Blue', 'CI', 'City', 'Class', 'DBSI', 'Green', 'NDSI2010', 'NDSI2015', 'NIR', 'NSAI1', 'NSAI2', 'RNDSI', 'Red', 'SWIR1', 'SWIR2', 'swirSoil', '.geo']\n","Total sample numbers in each classes :\n","0    10000\n","1    10000\n","2    10000\n","3    10000\n","4    10000\n","Name: Class, dtype: int64\n","n_estimators=250\n","\n","Accuracy: 0.7940955716787591, Feature: NIR\n","Accuracy: 0.7957584140156755, Feature: NSAI1\n","Accuracy: 0.8000935234977788, Feature: NDSI2015\n","Accuracy: 0.8094696107443661, Feature: NSAI2\n","Accuracy: 0.8101969872537659, Feature: CI\n","Accuracy: 0.814608048383345, Feature: RNDSI\n","Accuracy: 0.8301606922126082, Feature: Blue\n","Accuracy: 0.8326866012325953, Feature: NDSI2010\n","Accuracy: 0.835192069392813, Feature: Green\n","Accuracy: 0.8466533466533467, Feature: SWIR1\n","Accuracy: 0.8572759022118743, Feature: DBSI\n","Accuracy: 0.8610905502097213, Feature: SWIR2\n","Accuracy: 0.8615308397324746, Feature: Red\n","Accuracy: 0.9005158437730287, Feature: swirSoil\n","Accuracy: 0.9005158437730287 Combination: ['swirSoil']\n","Accuracy: 0.9345700676861369 Combination: ['swirSoil', 'NDSI2015']\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3b326596088b>\u001b[0m in \u001b[0;36m<cell line: 120>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m    \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-3b326596088b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;31m# Train the model and evaluate it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Store the accuracy of the feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-3b326596088b>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Train the model using the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Make predictions and calculate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mtransformed_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m     79\u001b[0m             )\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m     82\u001b[0m             delayed(_fit_single_estimator)(\n\u001b[1;32m     83\u001b[0m                 \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py\u001b[0m in \u001b[0;36m_fit_single_estimator\u001b[0;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1488\u001b[0m             )\n\u001b[1;32m   1489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1491\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1919\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m                                                     dtrain.handle))\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# 10th Sempember 2023\n","# Use XGB only\n","# Testing with 150k Samples\n","# n_estimator = 300\n","# Version 12\n","# improve the code with function\n","\n","# different Class will have different numbers of samples\n","# Now, instead RF classifier, this code uses Soft Voting ensemble classifier with RF and XGB classifier.\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","import pandas as pd\n","import xgboost as xgb\n","import numpy as np\n","\n","# Load the CSV file into a DataFrame\n","data = pd.read_csv('/content/drive/MyDrive/ELM/merged_CSV/merged_60K_Soil_Five_Classes.csv').dropna()\n","data = data.drop_duplicates(subset=['.geo'])\n","print('data.columns.tolist() :',data.columns.tolist())\n","\n","# # Define a function for model training and evaluation\n","# def train_and_evaluate(X_train, X_test, y_train, y_test):\n","#     # Create a pipeline for data preprocessing and model training\n","#     pipeline = Pipeline([\n","#         ('scaling', StandardScaler()),\n","#         ('classification', VotingClassifier(\n","#             estimators=[\n","#                 ('Random Forest', RandomForestClassifier(n_estimators=300, random_state=100, n_jobs=-1)),\n","#                 ('XGBoost', xgb.XGBClassifier(n_estimators=300, random_state=100, n_jobs=-1))\n","#             ],\n","#             voting='soft'\n","#         ))\n","#     ])\n","\n","#     # Train the model using the pipeline\n","#     pipeline.fit(X_train, y_train)\n","\n","#     # Make predictions and calculate accuracy\n","#     predictions = pipeline.predict(X_test)\n","#     accuracy = f1_score(y_test, predictions, average=None)[3] #3 is for soil class\n","\n","#     return accuracy\n","\n","# Define a function for model training and evaluation (XGB Only)\n","def train_and_evaluate(X_train, X_test, y_train, y_test):\n","    # Create a pipeline for data preprocessing and model training\n","    pipeline = Pipeline([\n","        ('scaling', StandardScaler()),\n","        ('classification', xgb.XGBClassifier(n_estimators=300, random_state=100, n_jobs=-1))\n","    ])\n","\n","    # Train the model using the pipeline\n","    pipeline.fit(X_train, y_train)\n","\n","    # Make predictions and calculate accuracy\n","    predictions = pipeline.predict(X_test)\n","    accuracy = f1_score(y_test, predictions, average=None)[3]\n","\n","    return accuracy\n","\n","# Define sample sizes\n","sample_sizes = {\n","    0: 10000,\n","    1: 10000,\n","    2: 10000,\n","    3: 10000,\n","    4: 10000\n","    }\n","\n","grouped = data.groupby('Class')# Group by class\n","sampled = grouped.apply(lambda x: x.sample(n=sample_sizes[x.name], replace=True).reset_index(drop=True))  # Sample from each group according to the defined sample sizes\n","sampled = [sampled] # Convert to list\n","data = pd.concat(sampled) # Concatenate samples\n","\n","target = data['Class']\n","data = data.drop(['Class','.geo','system:index', 'City'], axis=1)\n","\n","# Verify counts\n","print('Total sample numbers in each classes :')\n","print(target.value_counts())\n","print()\n","\n","# Assuming 'data' is your DataFrame and 'target' is the target variable\n","features = data.columns.tolist()\n","\n","# Record the best accuracy and feature combination\n","best_accuracy = 0\n","best_combination = []\n","\n","# Loop over all features\n","for _ in range(len(features)):\n","    best_feature = None\n","\n","    for feature in features:\n","        # Add the current feature to the best combination\n","        combination = best_combination + [feature]\n","\n","        # Split the data into training and test sets (with a random seed)\n","        X_train, X_test, y_train, y_test = train_test_split(data[combination], target, test_size=0.20, random_state=0)\n","\n","        # Train the model and evaluate it\n","        accuracy = train_and_evaluate(X_train, X_test, y_train, y_test)\n","\n","        # If this combination has the best accuracy so far, record it\n","        if accuracy > best_accuracy:\n","            best_accuracy = accuracy\n","            best_feature = feature\n","\n","    # Add the best feature of this iteration to the best combination\n","    if best_feature is not None:\n","        best_combination.append(best_feature)\n","        features.remove(best_feature)\n","\n","    print(f\"Accuracy: {best_accuracy}\", f\"Combination: {best_combination}\")\n","\n","print(f\"Best accuracy: {best_accuracy}\")\n","print(f\"Best feature combination: {best_combination}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":720},"id":"8nId7NkLnlVx","executionInfo":{"status":"error","timestamp":1694348595565,"user_tz":-480,"elapsed":110250,"user":{"displayName":"Akib Javed","userId":"01752818298668219500"}},"outputId":"745f2f77-b81e-4b37-97f3-a8b05fb61430"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["data.columns.tolist() : ['system:index', 'Blue', 'CI', 'City', 'Class', 'DBSI', 'Green', 'NDSI2010', 'NDSI2015', 'NIR', 'NSAI1', 'NSAI2', 'RNDSI', 'Red', 'SWIR1', 'SWIR2', 'swirSoil', '.geo']\n","Total sample numbers in each classes :\n","0    10000\n","1    10000\n","2    10000\n","3    10000\n","4    10000\n","Name: Class, dtype: int64\n","\n","Accuracy: 0.826995718962478 Combination: ['swirSoil']\n","Accuracy: 0.9090031378228336 Combination: ['swirSoil', 'NIR']\n","Accuracy: 0.9599802371541502 Combination: ['swirSoil', 'NIR', 'Green']\n","Accuracy: 0.9707776126795442 Combination: ['swirSoil', 'NIR', 'Green', 'CI']\n","Accuracy: 0.9747274529236869 Combination: ['swirSoil', 'NIR', 'Green', 'CI', 'NDSI2010']\n","Accuracy: 0.9747274529236869 Combination: ['swirSoil', 'NIR', 'Green', 'CI', 'NDSI2010']\n","Accuracy: 0.9747274529236869 Combination: ['swirSoil', 'NIR', 'Green', 'CI', 'NDSI2010']\n","Accuracy: 0.9747274529236869 Combination: ['swirSoil', 'NIR', 'Green', 'CI', 'NDSI2010']\n","Accuracy: 0.9747274529236869 Combination: ['swirSoil', 'NIR', 'Green', 'CI', 'NDSI2010']\n","Accuracy: 0.9747274529236869 Combination: ['swirSoil', 'NIR', 'Green', 'CI', 'NDSI2010']\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-c7ba46bbced9>\u001b[0m in \u001b[0;36m<cell line: 95>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# Train the model and evaluate it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# If this combination has the best accuracy so far, record it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-c7ba46bbced9>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Train the model using the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Make predictions and calculate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1488\u001b[0m             )\n\u001b[1;32m   1489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1491\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1919\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m                                                     dtrain.handle))\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WlJ0Zqk2xpoG"},"outputs":[],"source":["# 10th Sempember 2023\n","# Testing with 180k Samples\n","# n_estimator = 300\n","# attempt 11\n","# improve the code with function\n","\n","# different Class will have different numbers of samples\n","# Now, instead RF classifier, this code uses Soft Voting ensemble classifier with RF and XGB classifier.\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","import pandas as pd\n","import xgboost as xgb\n","import numpy as np\n","\n","# Load the CSV file into a DataFrame\n","# data = pd.read_csv('merged_182K_Chapter5_Five_Class_Model.csv')\n","data = pd.read_csv('/content/drive/MyDrive/ELM/merged_CSV/merged_60K_Soil_Five_Classes.csv').dropna()\n","data = data.drop_duplicates(subset=['.geo'])\n","\n","# # Define a function for model training and evaluation\n","# def train_and_evaluate(X_train, X_test, y_train, y_test):\n","#     # Create a pipeline for data preprocessing and model training\n","#     pipeline = Pipeline([\n","#         ('scaling', StandardScaler()),\n","#         ('classification', VotingClassifier(\n","#             estimators=[\n","#                 ('Random Forest', RandomForestClassifier(n_estimators=300, random_state=100, n_jobs=-1)),\n","#                 ('XGBoost', xgb.XGBClassifier(n_estimators=300, random_state=100, n_jobs=-1))\n","#             ],\n","#             voting='soft'\n","#         ))\n","#     ])\n","\n","#     # Rest of the function\n","#     pipeline.fit(X_train, y_train)\n","#     predictions = pipeline.predict(X_test)\n","#     accuracy = f1_score(y_test, predictions, average=None)[4]\n","\n","#     return accuracy\n","\n","# Define a function for model training and evaluation (XGB Only)\n","def train_and_evaluate(X_train, X_test, y_train, y_test):\n","    # Create a pipeline for data preprocessing and model training\n","    pipeline = Pipeline([\n","        ('scaling', StandardScaler()),\n","        ('classification', xgb.XGBClassifier(n_estimators=300, random_state=100, n_jobs=-1))\n","    ])\n","\n","    pipeline.fit(X_train, y_train)  # Train the model using the pipeline\n","    predictions = pipeline.predict(X_test)    # Make predictions and calculate accuracy\n","    accuracy = f1_score(y_test, predictions, average=None)[3]\n","\n","    return accuracy\n","\n","\n","# Define sample sizes\n","sample_sizes = {\n","    0: 10000,\n","    1: 10000,\n","    2: 10000,\n","    3: 10000,\n","    4: 10000\n","    }\n","\n","# Grouped, sampled and concentrated them together into single dataframe\n","grouped = data.groupby('Class')# Group by class\n","sampled = grouped.apply(lambda x: x.sample(n=sample_sizes[x.name], replace=True).reset_index(drop=True))  # Sample from each group according to the defined sample sizes\n","sampled = [sampled] # Convert to list\n","data = pd.concat(sampled) # Concatenate samples\n","\n","# select the terget data column\n","target = data['Class']\n","data = data.drop(['Class','.geo','system:index','City'], axis=1)\n","\n","# Verify counts\n","print('Total sample numbers in each classes :')\n","print(target.value_counts())\n","print()\n","\n","# Assuming 'data' is your DataFrame and 'target' is the target variable\n","features = data.columns.tolist()\n","\n","# Record the best accuracy and feature combination\n","best_accuracy = 0\n","best_combination = []\n","\n","# Loop over all features\n","for _ in range(len(features)):\n","    best_feature = None\n","\n","    # Standardize\n","    scaler = StandardScaler().fit(data)\n","    standardized_data = scaler.transform(data)\n","\n","    # Split\n","    X_train, X_test, y_train, y_test = train_test_split(standardized_data, target, test_size=0.20, random_state=0)\n","\n","    # Feature selection\n","    for feature in features:\n","\n","        # Pass standardized data\n","        accuracy = train_and_evaluate(X_train, X_test, y_train, y_test)\n","\n","        # Add the current feature to the best combination\n","        combination = best_combination + [feature]\n","\n","        # If this combination has the best accuracy so far, record it\n","        if accuracy > best_accuracy:\n","            best_accuracy = accuracy\n","            best_feature = feature\n","\n","    # Add the best feature of this iteration to the best combination\n","    if best_feature is not None:\n","        best_combination.append(best_feature)\n","        features.remove(best_feature)\n","\n","    print(f\"Accuracy: {best_accuracy}\", f\"Combination: {best_combination}\")\n","\n","print(f\"Best accuracy: {best_accuracy}\")\n","print(f\"Best feature combination: {best_combination}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HJDqVQU5GjbR","executionInfo":{"status":"ok","timestamp":1694341949775,"user_tz":-480,"elapsed":568168,"user":{"displayName":"Akib Javed","userId":"01752818298668219500"}},"outputId":"9bf3974e-3cbe-4e39-d444-149ab2421692"},"outputs":[{"output_type":"stream","name":"stdout","text":["data.columns.tolist() : ['system:index', 'Blue', 'CI', 'City', 'Class', 'DBSI', 'Green', 'NDSI2010', 'NDSI2015', 'NIR', 'NSAI1', 'NSAI2', 'RNDSI', 'Red', 'SWIR1', 'SWIR2', 'swirSoil', '.geo']\n","Total sample numbers in each classes :\n","0    10000\n","1    10000\n","2    10000\n","3    10000\n","4    10000\n","Name: Class, dtype: int64\n","\n","Accuracy: 0.9002457002457003 Combination: ['swirSoil']\n","Accuracy: 0.9405594405594406 Combination: ['swirSoil', 'NDSI2015']\n","Accuracy: 0.9636947394418374 Combination: ['swirSoil', 'NDSI2015', 'NDSI2010']\n","Accuracy: 0.9724223602484472 Combination: ['swirSoil', 'NDSI2015', 'NDSI2010', 'NSAI1']\n","Accuracy: 0.9768599154018413 Combination: ['swirSoil', 'NDSI2015', 'NDSI2010', 'NSAI1', 'NSAI2']\n","Accuracy: 0.9785322016974538 Combination: ['swirSoil', 'NDSI2015', 'NDSI2010', 'NSAI1', 'NSAI2', 'Red']\n","Accuracy: 0.9785322016974538 Combination: ['swirSoil', 'NDSI2015', 'NDSI2010', 'NSAI1', 'NSAI2', 'Red']\n","Accuracy: 0.9785322016974538 Combination: ['swirSoil', 'NDSI2015', 'NDSI2010', 'NSAI1', 'NSAI2', 'Red']\n","Accuracy: 0.9785322016974538 Combination: ['swirSoil', 'NDSI2015', 'NDSI2010', 'NSAI1', 'NSAI2', 'Red']\n","Accuracy: 0.9785322016974538 Combination: ['swirSoil', 'NDSI2015', 'NDSI2010', 'NSAI1', 'NSAI2', 'Red']\n","Accuracy: 0.9785322016974538 Combination: ['swirSoil', 'NDSI2015', 'NDSI2010', 'NSAI1', 'NSAI2', 'Red']\n","Accuracy: 0.9785322016974538 Combination: ['swirSoil', 'NDSI2015', 'NDSI2010', 'NSAI1', 'NSAI2', 'Red']\n","Accuracy: 0.9785322016974538 Combination: ['swirSoil', 'NDSI2015', 'NDSI2010', 'NSAI1', 'NSAI2', 'Red']\n","Accuracy: 0.9785322016974538 Combination: ['swirSoil', 'NDSI2015', 'NDSI2010', 'NSAI1', 'NSAI2', 'Red']\n","Best accuracy: 0.9785322016974538\n","Best feature combination: ['swirSoil', 'NDSI2015', 'NDSI2010', 'NSAI1', 'NSAI2', 'Red']\n"]}],"source":["# 10th Sempember 2023\n","# Testing with 150k Samples\n","# n_estimator = 300\n","# attempt 10\n","# improve the code with function\n","\n","# different Class will have different numbers of samples\n","# Now, instead RF classifier, this code uses Soft Voting ensemble classifier with RF and XGB classifier.\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","import pandas as pd\n","import xgboost as xgb\n","import numpy as np\n","\n","# Load the CSV file into a DataFrame\n","data = pd.read_csv('/content/drive/MyDrive/ELM/merged_CSV/merged_60K_Soil_Five_Classes.csv').dropna()\n","data = data.drop_duplicates(subset=['.geo'])\n","print('data.columns.tolist() :',data.columns.tolist())\n","\n","# Define a function for model training and evaluation\n","def train_and_evaluate(X_train, X_test, y_train, y_test):\n","    # Create a pipeline for data preprocessing and model training\n","    pipeline = Pipeline([\n","        ('scaling', StandardScaler()),\n","        ('classification', VotingClassifier(\n","            estimators=[\n","                ('Random Forest', RandomForestClassifier(n_estimators=300, random_state=100, n_jobs=-1)),\n","                ('XGBoost', xgb.XGBClassifier(n_estimators=300, random_state=100, n_jobs=-1))\n","            ],\n","            voting='soft'\n","        ))\n","    ])\n","\n","    # Train the model using the pipeline\n","    pipeline.fit(X_train, y_train)\n","\n","    # Make predictions and calculate accuracy\n","    predictions = pipeline.predict(X_test)\n","    accuracy = f1_score(y_test, predictions, average=None)[3] #3 is for soil class\n","\n","    return accuracy\n","\n","# Define sample sizes\n","sample_sizes = {\n","    0: 10000,\n","    1: 10000,\n","    2: 10000,\n","    3: 10000,\n","    4: 10000\n","    }\n","\n","grouped = data.groupby('Class')# Group by class\n","sampled = grouped.apply(lambda x: x.sample(n=sample_sizes[x.name], replace=True).reset_index(drop=True))  # Sample from each group according to the defined sample sizes\n","sampled = [sampled] # Convert to list\n","data = pd.concat(sampled) # Concatenate samples\n","\n","target = data['Class']\n","data = data.drop(['Class','.geo','system:index', 'City'], axis=1)\n","\n","# Verify counts\n","print('Total sample numbers in each classes :')\n","print(target.value_counts())\n","print()\n","\n","# Assuming 'data' is your DataFrame and 'target' is the target variable\n","features = data.columns.tolist()\n","\n","# Record the best accuracy and feature combination\n","best_accuracy = 0\n","best_combination = []\n","\n","# Loop over all features\n","for _ in range(len(features)):\n","    best_feature = None\n","\n","    for feature in features:\n","        # Add the current feature to the best combination\n","        combination = best_combination + [feature]\n","\n","        # Split the data into training and test sets (with a random seed)\n","        X_train, X_test, y_train, y_test = train_test_split(data[combination], target, test_size=0.20, random_state=0)\n","\n","        # Train the model and evaluate it\n","        accuracy = train_and_evaluate(X_train, X_test, y_train, y_test)\n","\n","        # If this combination has the best accuracy so far, record it\n","        if accuracy > best_accuracy:\n","            best_accuracy = accuracy\n","            best_feature = feature\n","\n","    # Add the best feature of this iteration to the best combination\n","    if best_feature is not None:\n","        best_combination.append(best_feature)\n","        features.remove(best_feature)\n","\n","    print(f\"Accuracy: {best_accuracy}\", f\"Combination: {best_combination}\")\n","\n","print(f\"Best accuracy: {best_accuracy}\")\n","print(f\"Best feature combination: {best_combination}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2809779,"status":"ok","timestamp":1694318566068,"user":{"displayName":"Akib Javed","userId":"01752818298668219500"},"user_tz":-480},"id":"P0g5FlHh-aRb","outputId":"9bc75ae6-951f-4cb7-a6f5-6b3f9c33e8cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total sample numbers in each classes :\n","4    5000\n","0    2500\n","1    2500\n","2    2500\n","3    2500\n","Name: Class, dtype: int64\n","\n","Accuracy: 0.7328671328671329 Combination: ['SISAI']\n","Accuracy: 0.8165225744476465 Combination: ['SISAI', 'L8_swir1']\n","Accuracy: 0.8684337349397591 Combination: ['SISAI', 'L8_swir1', 'mndwiMedian']\n","Accuracy: 0.8960155490767736 Combination: ['SISAI', 'L8_swir1', 'mndwiMedian', 'ndviMax']\n","Accuracy: 0.9092702169625245 Combination: ['SISAI', 'L8_swir1', 'mndwiMedian', 'ndviMax', 'NSAI2']\n","Accuracy: 0.9158050221565731 Combination: ['SISAI', 'L8_swir1', 'mndwiMedian', 'ndviMax', 'NSAI2', 'ndviMedian']\n","Accuracy: 0.9193706981317601 Combination: ['SISAI', 'L8_swir1', 'mndwiMedian', 'ndviMax', 'NSAI2', 'ndviMedian', 'L8_blue']\n","Accuracy: 0.9239823442864149 Combination: ['SISAI', 'L8_swir1', 'mndwiMedian', 'ndviMax', 'NSAI2', 'ndviMedian', 'L8_blue', 'swirSoil']\n","Accuracy: 0.9239823442864149 Combination: ['SISAI', 'L8_swir1', 'mndwiMedian', 'ndviMax', 'NSAI2', 'ndviMedian', 'L8_blue', 'swirSoil']\n","Accuracy: 0.9239823442864149 Combination: ['SISAI', 'L8_swir1', 'mndwiMedian', 'ndviMax', 'NSAI2', 'ndviMedian', 'L8_blue', 'swirSoil']\n","Accuracy: 0.9239823442864149 Combination: ['SISAI', 'L8_swir1', 'mndwiMedian', 'ndviMax', 'NSAI2', 'ndviMedian', 'L8_blue', 'swirSoil']\n","Accuracy: 0.9239823442864149 Combination: ['SISAI', 'L8_swir1', 'mndwiMedian', 'ndviMax', 'NSAI2', 'ndviMedian', 'L8_blue', 'swirSoil']\n","Accuracy: 0.9239823442864149 Combination: ['SISAI', 'L8_swir1', 'mndwiMedian', 'ndviMax', 'NSAI2', 'ndviMedian', 'L8_blue', 'swirSoil']\n","Accuracy: 0.9239823442864149 Combination: ['SISAI', 'L8_swir1', 'mndwiMedian', 'ndviMax', 'NSAI2', 'ndviMedian', 'L8_blue', 'swirSoil']\n","Accuracy: 0.9239823442864149 Combination: ['SISAI', 'L8_swir1', 'mndwiMedian', 'ndviMax', 'NSAI2', 'ndviMedian', 'L8_blue', 'swirSoil']\n","Accuracy: 0.9239823442864149 Combination: ['SISAI', 'L8_swir1', 'mndwiMedian', 'ndviMax', 'NSAI2', 'ndviMedian', 'L8_blue', 'swirSoil']\n","Accuracy: 0.9239823442864149 Combination: ['SISAI', 'L8_swir1', 'mndwiMedian', 'ndviMax', 'NSAI2', 'ndviMedian', 'L8_blue', 'swirSoil']\n","Accuracy: 0.9239823442864149 Combination: ['SISAI', 'L8_swir1', 'mndwiMedian', 'ndviMax', 'NSAI2', 'ndviMedian', 'L8_blue', 'swirSoil']\n","Best accuracy: 0.9239823442864149\n","Best feature combination: ['SISAI', 'L8_swir1', 'mndwiMedian', 'ndviMax', 'NSAI2', 'ndviMedian', 'L8_blue', 'swirSoil']\n"]}],"source":["# 10th Sempember 2023\n","# Testing with 15k samples\n","# attempt 9\n","# improve the code with function\n","\n","# different Class will have different numbers of samples\n","# Now, instead RF classifier, this code uses Soft Voting ensemble classifier with RF and XGB classifier.\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","import pandas as pd\n","import xgboost as xgb\n","import numpy as np\n","\n","# Load the CSV file into a DataFrame\n","data = pd.read_csv('/content/drive/MyDrive/Chapter5Model//merged_CSV/merged_162K_Chapter5_Five_Class_Model.csv').dropna()\n","data = data.drop_duplicates(subset=['.geo'])\n","\n","# # Define a function for model training and evaluation\n","# def train_and_evaluate(X_train, X_test, y_train, y_test):\n","#     # Create a pipeline for data preprocessing and model training\n","#     pipeline = Pipeline([\n","#         ('scaling', StandardScaler()),\n","#         ('classification', VotingClassifier(\n","#             estimators=[\n","#                 ('Random Forest', RandomForestClassifier(n_estimators=500, random_state=100)),\n","#                 ('XGBoost', xgb.XGBClassifier(n_estimators=500, random_state=100))\n","#             ],\n","#             voting='soft'\n","#         ))\n","#     ])\n","\n","# Define a function for model training and evaluation\n","def train_and_evaluate(X_train, X_test, y_train, y_test):\n","    # Create a pipeline for data preprocessing and model training\n","    pipeline = Pipeline([\n","        ('scaling', StandardScaler()),\n","        ('classification', VotingClassifier(\n","            estimators=[\n","                ('Random Forest', RandomForestClassifier(n_estimators=100, random_state=100, n_jobs=-1)),\n","                ('XGBoost', xgb.XGBClassifier(n_estimators=100, random_state=100, n_jobs=-1))\n","            ],\n","            voting='soft'\n","        ))\n","    ])\n","\n","    # Train the model using the pipeline\n","    pipeline.fit(X_train, y_train)\n","\n","    # Make predictions and calculate accuracy\n","    predictions = pipeline.predict(X_test)\n","    accuracy = f1_score(y_test, predictions, average=None)[4]\n","\n","    return accuracy\n","\n","# Define sample sizes\n","sample_sizes = {\n","    0: 2500,\n","    1: 2500,\n","    2: 2500,\n","    3: 2500,\n","    4: 5000\n","    }\n","\n","grouped = data.groupby('Class')# Group by class\n","sampled = grouped.apply(lambda x: x.sample(n=sample_sizes[x.name], replace=True).reset_index(drop=True))  # Sample from each group according to the defined sample sizes\n","sampled = [sampled] # Convert to list\n","data = pd.concat(sampled) # Concatenate samples\n","\n","target = data['Class']\n","data = data.drop(['Class','.geo','system:index','City', 'NDBI', 'UI', 'SwiRed', 'INDBI', 'NBAI', 'BLFEI'], axis=1)\n","\n","# Verify counts\n","print('Total sample numbers in each classes :')\n","print(target.value_counts())\n","print()\n","\n","# Assuming 'data' is your DataFrame and 'target' is the target variable\n","features = data.columns.tolist()\n","\n","# Record the best accuracy and feature combination\n","best_accuracy = 0\n","best_combination = []\n","\n","# Loop over all features\n","for _ in range(len(features)):\n","    best_feature = None\n","\n","    for feature in features:\n","        # Add the current feature to the best combination\n","        combination = best_combination + [feature]\n","\n","        # Split the data into training and test sets (with a random seed)\n","        X_train, X_test, y_train, y_test = train_test_split(data[combination], target, test_size=0.20, random_state=0)\n","\n","        # Train the model and evaluate it\n","        accuracy = train_and_evaluate(X_train, X_test, y_train, y_test)\n","\n","        # If this combination has the best accuracy so far, record it\n","        if accuracy > best_accuracy:\n","            best_accuracy = accuracy\n","            best_feature = feature\n","\n","    # Add the best feature of this iteration to the best combination\n","    if best_feature is not None:\n","        best_combination.append(best_feature)\n","        features.remove(best_feature)\n","\n","    print(f\"Accuracy: {best_accuracy}\", f\"Combination: {best_combination}\")\n","\n","print(f\"Best accuracy: {best_accuracy}\")\n","print(f\"Best feature combination: {best_combination}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3024,"status":"ok","timestamp":1694313112052,"user":{"displayName":"Akib Javed","userId":"01752818298668219500"},"user_tz":-480},"id":"ACciCEFu677l","outputId":"b758aa75-2e01-4494-e28b-234297d531e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["4    61752\n","2    39673\n","3    29236\n","1    28119\n","0     3247\n","Name: Class, dtype: int64\n"]}],"source":["import pandas as pd\n","\n","# Load the CSV file into a DataFrame\n","data = pd.read_csv('/content/drive/MyDrive/Chapter5Model//merged_CSV/merged_162K_Chapter5_Five_Class_Model.csv').dropna()\n","data = data.drop_duplicates(subset=['.geo'])\n","\n","print(data['Class'].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":529},"executionInfo":{"elapsed":2339760,"status":"error","timestamp":1694315508997,"user":{"displayName":"Akib Javed","userId":"01752818298668219500"},"user_tz":-480},"id":"lgNrI0WW12lY","outputId":"f744bc8e-6826-456d-81c4-45294fa8c72d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total sample numbers in each classes :\n","4    50000\n","0    25000\n","1    25000\n","2    25000\n","3    25000\n","Name: Class, dtype: int64\n","\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-70decf569469>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m         )\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mensemble_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# Make predictions and calculate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mtransformed_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m     79\u001b[0m             )\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m     82\u001b[0m             delayed(_fit_single_estimator)(\n\u001b[1;32m     83\u001b[0m                 \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py\u001b[0m in \u001b[0;36m_fit_single_estimator\u001b[0;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# 10th September 2023\n","# attempt 8\n","# different Class will have different numbers of samples\n","# Now, instead RF classifier, this code uses Soft Voting ensemble classifier with RF and XGB classifier.\n","\n","import itertools\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.preprocessing import StandardScaler\n","import pandas as pd\n","import xgboost as xgb\n","\n","# Load the CSV file into a DataFrame\n","data = pd.read_csv('/content/drive/MyDrive/Chapter5Model//merged_CSV/merged_162K_Chapter5_Five_Class_Model.csv').dropna()\n","data = data.drop_duplicates(subset=['.geo'])\n","\n","# Define sample sizes\n","sample_sizes = {\n","    0: 25000,\n","    1: 25000,\n","    2: 25000,\n","    3: 25000,\n","    4: 50000\n","    }\n","\n","grouped = data.groupby('Class')# Group by class\n","sampled = grouped.apply(lambda x: x.sample(n=sample_sizes[x.name], replace=True).reset_index(drop=True))  # Sample from each group according to the defined sample sizes\n","sampled = [sampled] # Convert to list\n","data = pd.concat(sampled) # Concatenate samples\n","\n","target = data['Class']\n","data = data.drop(['Class','.geo','system:index','City', 'NDBI', 'UI', 'SwiRed', 'INDBI', 'NBAI', 'BLFEI'], axis=1)\n","\n","# Verify counts\n","print('Total sample numbers in each classes :')\n","print(target.value_counts())\n","print()\n","\n","# Assuming 'data' is your DataFrame and 'target' is the target variable\n","features = data.columns.tolist()\n","\n","# Record the best accuracy and feature combination\n","best_accuracy = 0\n","best_combination = []\n","\n","# Loop over all features\n","for _ in range(len(features)):\n","    best_feature = None\n","\n","    for feature in features:\n","        # Add the current feature to the best combination\n","        combination = best_combination + [feature]\n","\n","        # Split the data into training and test sets\n","        X_train, X_test, y_train, y_test = train_test_split(data[combination], target, test_size=0.20)\n","\n","        # Normalize the data\n","        scaler = StandardScaler()\n","        X_train = scaler.fit_transform(X_train)\n","        X_test = scaler.fit_transform(X_test)\n","\n","        # Train the model using Random Forest and XGBoost classifiers\n","        rf_model = RandomForestClassifier(n_estimators=500, random_state=100)\n","        xgb_model = xgb.XGBClassifier(n_estimators=500, random_state=100)\n","\n","        ensemble_model = VotingClassifier(\n","            estimators=[\n","                ('Random Forest', rf_model),\n","                ('XGBoost', xgb_model)\n","            ],\n","            voting='soft'\n","        )\n","\n","        ensemble_model.fit(X_train, y_train)\n","\n","        # Make predictions and calculate accuracy\n","        predictions = ensemble_model.predict(X_test)\n","        rf_f1 = f1_score(y_test, predictions, average=None)[4]  # the last \"[4]\" is for the impervious surface area. This is the fifth element of the accuracy matrix. hence the 4th element of the index.\n","\n","        # If this combination has the best accuracy so far, record it\n","        if rf_f1 > best_accuracy:\n","            best_accuracy = rf_f1\n","            best_feature = feature\n","\n","    # Add the best feature of this iteration to the best combination\n","    if best_feature is not None:\n","        best_combination.append(best_feature)\n","        features.remove(best_feature)\n","\n","    print(f\"Accuracy: {best_accuracy}\", f\"Combination: {best_combination}\")\n","\n","print(f\"Best accuracy: {best_accuracy}\")\n","print(f\"Best feature combination: {best_combination}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":807},"executionInfo":{"elapsed":878729,"status":"error","timestamp":1694312918315,"user":{"displayName":"Akib Javed","userId":"01752818298668219500"},"user_tz":-480},"id":"XEejcO_l0ieG","outputId":"0b8dd097-851a-4569-e22b-e86583ae5509"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total sample numbers in each classes :\n","0    1000\n","1    1000\n","2    1000\n","3    1000\n","4    1000\n","Name: Class, dtype: int64\n","\n","Accuracy: 0.5775193798449613 Combination: ['SISAI']\n","Accuracy: 0.735632183908046 Combination: ['SISAI', 'L8_swir2']\n","Accuracy: 0.7907869481765836 Combination: ['SISAI', 'L8_swir2', 'L8_nir']\n","Accuracy: 0.80859375 Combination: ['SISAI', 'L8_swir2', 'L8_nir', 'ndviMax']\n","Accuracy: 0.848030018761726 Combination: ['SISAI', 'L8_swir2', 'L8_nir', 'ndviMax', 'nduiMin']\n","Accuracy: 0.8637992831541218 Combination: ['SISAI', 'L8_swir2', 'L8_nir', 'ndviMax', 'nduiMin', 'ndbiMin']\n","Accuracy: 0.8637992831541218 Combination: ['SISAI', 'L8_swir2', 'L8_nir', 'ndviMax', 'nduiMin', 'ndbiMin']\n","Accuracy: 0.8740458015267176 Combination: ['SISAI', 'L8_swir2', 'L8_nir', 'ndviMax', 'nduiMin', 'ndbiMin', 'NSAI1']\n","Accuracy: 0.8740458015267176 Combination: ['SISAI', 'L8_swir2', 'L8_nir', 'ndviMax', 'nduiMin', 'ndbiMin', 'NSAI1']\n","Accuracy: 0.8740458015267176 Combination: ['SISAI', 'L8_swir2', 'L8_nir', 'ndviMax', 'nduiMin', 'ndbiMin', 'NSAI1']\n","Accuracy: 0.8740458015267176 Combination: ['SISAI', 'L8_swir2', 'L8_nir', 'ndviMax', 'nduiMin', 'ndbiMin', 'NSAI1']\n","Accuracy: 0.8806262230919766 Combination: ['SISAI', 'L8_swir2', 'L8_nir', 'ndviMax', 'nduiMin', 'ndbiMin', 'NSAI1', 'mndwiMedian']\n","Accuracy: 0.8806262230919766 Combination: ['SISAI', 'L8_swir2', 'L8_nir', 'ndviMax', 'nduiMin', 'ndbiMin', 'NSAI1', 'mndwiMedian']\n","Accuracy: 0.8806262230919766 Combination: ['SISAI', 'L8_swir2', 'L8_nir', 'ndviMax', 'nduiMin', 'ndbiMin', 'NSAI1', 'mndwiMedian']\n","Accuracy: 0.8937007874015748 Combination: ['SISAI', 'L8_swir2', 'L8_nir', 'ndviMax', 'nduiMin', 'ndbiMin', 'NSAI1', 'mndwiMedian', 'NSAI2']\n","Accuracy: 0.8937007874015748 Combination: ['SISAI', 'L8_swir2', 'L8_nir', 'ndviMax', 'nduiMin', 'ndbiMin', 'NSAI1', 'mndwiMedian', 'NSAI2']\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-3a4d6b830e48>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         )\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mensemble_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Make predictions and calculate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mtransformed_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m     79\u001b[0m             )\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m     82\u001b[0m             delayed(_fit_single_estimator)(\n\u001b[1;32m     83\u001b[0m                 \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py\u001b[0m in \u001b[0;36m_fit_single_estimator\u001b[0;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1488\u001b[0m             )\n\u001b[1;32m   1489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1491\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1919\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m                                                     dtrain.handle))\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# attempt 7\n","# Now, instead RF classifier, this code uses Soft Voting ensemble classifier with RF and XGB classifier.\n","\n","import itertools\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","import pandas as pd\n","import xgboost as xgb\n","\n","# Load the CSV file into a DataFrame\n","data = pd.read_csv('/content/drive/MyDrive/Chapter5Model//merged_CSV/merged_162K_Chapter5_Five_Class_Model.csv').dropna()\n","\n","n = 1000 # Define sample size\n","grouped = data.groupby('Class') # Group by class\n","sampled = grouped.apply(lambda x: x.sample(n=n, replace=True).reset_index(drop=True)) # Sample from each group\n","sampled = [sampled] # Convert to list\n","data = pd.concat(sampled)  # Concatenate samples\n","\n","target = data['Class']\n","data = data.drop(['Class','.geo','system:index','City', 'NDBI', 'UI', 'SwiRed', 'INDBI', 'NBAI', 'BLFEI'], axis=1)\n","\n","# Verify counts\n","print('Total sample numbers in each classes :')\n","print(target.value_counts())\n","print()\n","\n","# Assuming 'data' is your DataFrame and 'target' is the target variable\n","features = data.columns.tolist()\n","\n","# Record the best accuracy and feature combination\n","best_accuracy = 0\n","best_combination = []\n","\n","# Loop over all features\n","for _ in range(len(features)):\n","    best_feature = None\n","\n","    for feature in features:\n","        # Add the current feature to the best combination\n","        combination = best_combination + [feature]\n","\n","        # Split the data into training and test sets\n","        X_train, X_test, y_train, y_test = train_test_split(data[combination], target, test_size=0.25)\n","\n","        # Train the model using Random Forest and XGBoost classifiers\n","        rf_model = RandomForestClassifier(n_estimators=100, random_state=100)\n","        xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=100)\n","\n","        ensemble_model = VotingClassifier(\n","            estimators=[\n","                ('Random Forest', rf_model),\n","                ('XGBoost', xgb_model)\n","            ],\n","            voting='soft'\n","        )\n","\n","        ensemble_model.fit(X_train, y_train)\n","\n","        # Make predictions and calculate accuracy\n","        predictions = ensemble_model.predict(X_test)\n","        rf_f1 = f1_score(y_test, predictions, average=None)[4]  # the last \"[4]\" is for the impervious surface area. This is the fifth element of the accuracy matrix. hence the 4th element of the index.\n","\n","        # If this combination has the best accuracy so far, record it\n","        if rf_f1 > best_accuracy:\n","            best_accuracy = rf_f1\n","            best_feature = feature\n","\n","    # Add the best feature of this iteration to the best combination\n","    if best_feature is not None:\n","        best_combination.append(best_feature)\n","        features.remove(best_feature)\n","\n","    print(f\"Accuracy: {best_accuracy}\", f\"Combination: {best_combination}\")\n","\n","print(f\"Best accuracy: {best_accuracy}\")\n","print(f\"Best feature combination: {best_combination}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"elapsed":1163506,"status":"error","timestamp":1694311790078,"user":{"displayName":"Akib Javed","userId":"01752818298668219500"},"user_tz":-480},"id":"xzhQ0uR8vj7N","outputId":"8a426fae-f88c-45d0-eaad-fbbe05ecbf52"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total sample numbers in each classes :\n","0    24000\n","1    24000\n","2    24000\n","3    24000\n","4    24000\n","Name: Class, dtype: int64\n","\n","Combination: ['SISAI'] Accuracy: 0.6767376729186115\n","Combination: ['SISAI', 'swirSoil'] Accuracy: 0.793420828362025\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-9564eac4b3d5>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Make predictions and calculate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Attempt 6\n","# Professional grade output with 24k samples in each class. Total 120k samples\n","# Only using RF model\n","# Now in each iteration one best contribution bands will be added to the best_combination.\n","\n","import itertools\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","from sklearn.ensemble import RandomForestClassifier\n","import pandas as pd\n","\n","# Load the CSV file into a DataFrame\n","data = pd.read_csv('/content/drive/MyDrive/Chapter5Model//merged_CSV/merged_162K_Chapter5_Five_Class_Model.csv').dropna()\n","\n","n = 24000 # Define sample size\n","grouped = data.groupby('Class') # Group by class\n","sampled = grouped.apply(lambda x: x.sample(n=n, replace=True).reset_index(drop=True)) # Sample from each group\n","sampled = [sampled] # Convert to list\n","data = pd.concat(sampled)  # Concatenate samples\n","\n","target = data['Class']\n","data = data.drop(['Class','.geo','system:index','City', 'NDBI', 'UI', 'SwiRed', 'INDBI', 'NBAI', 'BLFEI'], axis=1)\n","\n","# Verify counts\n","print('Total sample numbers in each classes :')\n","print(target.value_counts())\n","print()\n","\n","# Assuming 'data' is your DataFrame and 'target' is the target variable\n","features = data.columns.tolist()\n","\n","# Record the best accuracy and feature combination\n","best_accuracy = 0\n","best_combination = []\n","\n","# Loop over all features\n","for _ in range(len(features)):\n","    best_feature = None\n","\n","    for feature in features:\n","        # Add the current feature to the best combination\n","        combination = best_combination + [feature]\n","\n","        # Split the data into training and test sets\n","        X_train, X_test, y_train, y_test = train_test_split(data[combination], target, test_size=0.25)\n","\n","        # Train the model\n","        model = RandomForestClassifier()\n","        model.fit(X_train, y_train)\n","\n","        # Make predictions and calculate accuracy\n","        predictions = model.predict(X_test)\n","        rf_f1 = f1_score(y_test, predictions, average=None)[4]  # the last \"[4]\" is for the impervious surface area. This is the fifth element of the accuracy matrix. hence the 4th element of the index.\n","\n","        # If this combination has the best accuracy so far, record it\n","        if rf_f1 > best_accuracy:\n","            best_accuracy = rf_f1\n","            best_feature = feature\n","\n","    # Add the best feature of this iteration to the best combination\n","    if best_feature is not None:\n","        best_combination.append(best_feature)\n","        features.remove(best_feature)\n","\n","    print(f\"Combination: {best_combination}\",f\"Accuracy: {best_accuracy}\")\n","\n","print(f\"Best accuracy: {best_accuracy}\")\n","print(f\"Best feature combination: {best_combination}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":681858,"status":"ok","timestamp":1694310625406,"user":{"displayName":"Akib Javed","userId":"01752818298668219500"},"user_tz":-480},"id":"HTKY56J_szA9","outputId":"6b2dab23-7958-4a97-b984-d7b30e92d853"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total sample numbers in each classes :\n","0    3000\n","1    3000\n","2    3000\n","3    3000\n","4    3000\n","Name: Class, dtype: int64\n","\n","Combination: ['SISAI'] Accuracy: 0.5288919102651258\n","Combination: ['SISAI', 'ndbiMin'] Accuracy: 0.6964980544747081\n","Combination: ['SISAI', 'ndbiMin', 'NSAI2'] Accuracy: 0.782608695652174\n","Combination: ['SISAI', 'ndbiMin', 'NSAI2', 'mndwiMedian'] Accuracy: 0.8215873015873015\n","Combination: ['SISAI', 'ndbiMin', 'NSAI2', 'mndwiMedian', 'ndviMedian'] Accuracy: 0.8464673913043478\n","Combination: ['SISAI', 'ndbiMin', 'NSAI2', 'mndwiMedian', 'ndviMedian', 'L8_swir1'] Accuracy: 0.8695652173913043\n","Combination: ['SISAI', 'ndbiMin', 'NSAI2', 'mndwiMedian', 'ndviMedian', 'L8_swir1', 'ndbiMedian'] Accuracy: 0.8810289389067524\n","Combination: ['SISAI', 'ndbiMin', 'NSAI2', 'mndwiMedian', 'ndviMedian', 'L8_swir1', 'ndbiMedian'] Accuracy: 0.8810289389067524\n","Combination: ['SISAI', 'ndbiMin', 'NSAI2', 'mndwiMedian', 'ndviMedian', 'L8_swir1', 'ndbiMedian'] Accuracy: 0.8810289389067524\n","Combination: ['SISAI', 'ndbiMin', 'NSAI2', 'mndwiMedian', 'ndviMedian', 'L8_swir1', 'ndbiMedian'] Accuracy: 0.8810289389067524\n","Combination: ['SISAI', 'ndbiMin', 'NSAI2', 'mndwiMedian', 'ndviMedian', 'L8_swir1', 'ndbiMedian', 'nduiMin'] Accuracy: 0.8868175765645806\n","Combination: ['SISAI', 'ndbiMin', 'NSAI2', 'mndwiMedian', 'ndviMedian', 'L8_swir1', 'ndbiMedian', 'nduiMin'] Accuracy: 0.8868175765645806\n","Combination: ['SISAI', 'ndbiMin', 'NSAI2', 'mndwiMedian', 'ndviMedian', 'L8_swir1', 'ndbiMedian', 'nduiMin'] Accuracy: 0.8868175765645806\n","Combination: ['SISAI', 'ndbiMin', 'NSAI2', 'mndwiMedian', 'ndviMedian', 'L8_swir1', 'ndbiMedian', 'nduiMin', 'L8_red'] Accuracy: 0.8878441907320349\n","Combination: ['SISAI', 'ndbiMin', 'NSAI2', 'mndwiMedian', 'ndviMedian', 'L8_swir1', 'ndbiMedian', 'nduiMin', 'L8_red'] Accuracy: 0.8878441907320349\n","Combination: ['SISAI', 'ndbiMin', 'NSAI2', 'mndwiMedian', 'ndviMedian', 'L8_swir1', 'ndbiMedian', 'nduiMin', 'L8_red'] Accuracy: 0.8878441907320349\n","Combination: ['SISAI', 'ndbiMin', 'NSAI2', 'mndwiMedian', 'ndviMedian', 'L8_swir1', 'ndbiMedian', 'nduiMin', 'L8_red'] Accuracy: 0.8878441907320349\n","Combination: ['SISAI', 'ndbiMin', 'NSAI2', 'mndwiMedian', 'ndviMedian', 'L8_swir1', 'ndbiMedian', 'nduiMin', 'L8_red'] Accuracy: 0.8878441907320349\n","Best accuracy: 0.8878441907320349\n","Best feature combination: ['SISAI', 'ndbiMin', 'NSAI2', 'mndwiMedian', 'ndviMedian', 'L8_swir1', 'ndbiMedian', 'nduiMin', 'L8_red']\n"]}],"source":["# Attempt 5\n","# Now in each iteration one best contribution bands will be added to the best_combination.\n","\n","import itertools\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","from sklearn.ensemble import RandomForestClassifier\n","import pandas as pd\n","\n","# Load the CSV file into a DataFrame\n","data = pd.read_csv('/content/drive/MyDrive/Chapter5Model//merged_CSV/merged_162K_Chapter5_Five_Class_Model.csv').dropna()\n","\n","n = 3000 # Define sample size\n","grouped = data.groupby('Class') # Group by class\n","sampled = grouped.apply(lambda x: x.sample(n=n, replace=False).reset_index(drop=True)) # Sample from each group\n","sampled = [sampled] # Convert to list\n","data = pd.concat(sampled)  # Concatenate samples\n","\n","target = data['Class']\n","data = data.drop(['Class','.geo','system:index','City', 'NDBI', 'UI', 'SwiRed', 'INDBI', 'NBAI', 'BLFEI'], axis=1)\n","\n","# Verify counts\n","print('Total sample numbers in each classes :')\n","print(target.value_counts())\n","print()\n","\n","# Assuming 'data' is your DataFrame and 'target' is the target variable\n","features = data.columns.tolist()\n","\n","# Record the best accuracy and feature combination\n","best_accuracy = 0\n","best_combination = []\n","\n","# Loop over all features\n","for _ in range(len(features)):\n","    best_feature = None\n","\n","    for feature in features:\n","        # Add the current feature to the best combination\n","        combination = best_combination + [feature]\n","\n","        # Split the data into training and test sets\n","        X_train, X_test, y_train, y_test = train_test_split(data[combination], target, test_size=0.25)\n","\n","        # Train the model\n","        model = RandomForestClassifier()\n","        model.fit(X_train, y_train)\n","\n","        # Make predictions and calculate accuracy\n","        predictions = model.predict(X_test)\n","        rf_f1 = f1_score(y_test, predictions, average=None)[4]  # the last \"[4]\" is for the impervious surface area. This is the fifth element of the accuracy matrix. hence the 4th element of the index.\n","\n","        # If this combination has the best accuracy so far, record it\n","        if rf_f1 > best_accuracy:\n","            best_accuracy = rf_f1\n","            best_feature = feature\n","\n","    # Add the best feature of this iteration to the best combination\n","    if best_feature is not None:\n","        best_combination.append(best_feature)\n","        features.remove(best_feature)\n","\n","    print(f\"Combination: {best_combination}\",f\"Accuracy: {best_accuracy}\")\n","\n","print(f\"Best accuracy: {best_accuracy}\")\n","print(f\"Best feature combination: {best_combination}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":442},"executionInfo":{"elapsed":594455,"status":"error","timestamp":1694309275213,"user":{"displayName":"Akib Javed","userId":"01752818298668219500"},"user_tz":-480},"id":"uK7MYPDwmr1-","outputId":"17e300e4-d261-4b89-df94-cff2e5ccf0a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Combination: ('swirSoil',) Accuracy: 0.5046728971962617\n","Combination: ('L8_swir1', 'ndviMax') Accuracy: 0.7398568019093078\n","Combination: ('L8_nir', 'L8_swir2', 'ndviMax') Accuracy: 0.8151898734177214\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-bd17df1bb3d9>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Make predictions and calculate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# best decide best model based on F1-Score of Impervious class.\n","# Attempt 4\n","\n","import itertools\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.ensemble import RandomForestClassifier\n","import pandas as pd\n","\n","# Load the CSV file into a DataFrame\n","data = pd.read_csv('/content/drive/MyDrive/Chapter5Model//merged_CSV/merged_162K_Chapter5_Five_Class_Model.csv').dropna()\n","\n","# Assuming 'target' is the name of the target variable column\n","\n","n = 1000 # Define sample size\n","grouped = data.groupby('Class') # Group by class\n","sampled = grouped.apply(lambda x: x.sample(n=n, replace=False).reset_index(drop=True)) # Sample from each group\n","sampled = [sampled] # Convert to list\n","data = pd.concat(sampled)  # Concatenate samples\n","\n","target = data['Class']\n","data = data.drop(['Class','.geo','system:index','City', 'NDBI', 'UI', 'SwiRed', 'INDBI', 'NBAI', 'BLFEI'], axis=1)\n","\n","# Assuming 'data' is your DataFrame and 'target' is the target variable\n","features = data.columns.tolist()\n","\n","# Record the best accuracy and feature combination\n","best_accuracy = 0\n","best_combination = None\n","\n","# Loop over all possible combinations of features\n","for r in range(1, len(features) + 1):\n","    for combination in itertools.combinations(features, r):\n","        # Split the data into training and test sets\n","        X_train, X_test, y_train, y_test = train_test_split(data[list(combination)], target, test_size=0.25)\n","\n","        # Train the model\n","        model = RandomForestClassifier()\n","        model.fit(X_train, y_train)\n","\n","        # Make predictions and calculate accuracy\n","        predictions = model.predict(X_test)\n","        # accuracy = accuracy_score(y_test, predictions)\n","        rf_f1 = f1_score(y_test, predictions, average=None)[4]  # the last \"[4]\" is for the impervious surface area. This is the fifth element of the accuracy matrix. hence the 4th element of the index.\n","\n","        # If this combination has the best accuracy so far, record it\n","        if rf_f1 > best_accuracy:\n","            best_accuracy = rf_f1\n","            best_combination = combination\n","    print(f\"Combination: {best_combination}\",f\"Accuracy: {best_accuracy}\")\n","\n","print(f\"Best accuracy: {best_accuracy}\")\n","print(f\"Best feature combination: {best_combination}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36353,"status":"ok","timestamp":1694307474685,"user":{"displayName":"Akib Javed","userId":"01752818298668219500"},"user_tz":-480},"id":"4UB8is_rl7Ti","outputId":"105ca58d-3777-4c99-8d7b-c85420326f44"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BZiOB94dpncB"},"outputs":[],"source":["# Five class training and best accuracy depends on the Overall Accuracy. Later model focus Impervious class only\n","# attempt 2\n","import itertools\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.ensemble import RandomForestClassifier\n","import pandas as pd\n","\n","# Load the CSV file into a DataFrame\n","data = pd.read_csv('/content/drive/MyDrive/Chapter5Model//merged_CSV/merged_162K_Chapter5_Five_Class_Model.csv').dropna()\n","\n","# Assuming 'target' is the name of the target variable column\n","\n","n = 2000 # Define sample size\n","grouped = data.groupby('Class') # Group by class\n","sampled = grouped.apply(lambda x: x.sample(n=n, replace=True).reset_index(drop=True)) # Sample from each group\n","sampled = [sampled] # Convert to list\n","data = pd.concat(sampled)  # Concatenate samples\n","\n","target = data['Class']\n","data = data.drop(['Class','.geo','system:index','City', 'NDBI', 'UI', 'SwiRed', 'INDBI', 'NBAI', 'BLFEI'], axis=1)\n","\n","# Assuming 'data' is your DataFrame and 'target' is the target variable\n","features = data.columns.tolist()\n","\n","# Record the best accuracy and feature combination\n","best_accuracy = 0\n","best_combination = None\n","\n","# Loop over all possible combinations of features\n","for r in range(1, len(features) + 1):\n","    for combination in itertools.combinations(features, r):\n","        # Split the data into training and test sets\n","        X_train, X_test, y_train, y_test = train_test_split(data[list(combination)], target, test_size=0.2)\n","\n","        # Train the model\n","        model = RandomForestClassifier()\n","        model.fit(X_train, y_train)\n","\n","        # Make predictions and calculate accuracy\n","        predictions = model.predict(X_test)\n","        accuracy = accuracy_score(y_test, predictions)\n","\n","        # If this combination has the best accuracy so far, record it\n","        if accuracy > best_accuracy:\n","            best_accuracy = accuracy\n","            best_combination = combination\n","    print(f\"Combination: {best_combination}\",f\"Accuracy: {best_accuracy}\")\n","\n","print(f\"Best accuracy: {best_accuracy}\")\n","print(f\"Best feature combination: {best_combination}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rh95KxWZpGzC"},"outputs":[],"source":["# Code using cross velidation\n","# Attempt 3\n","\n","import itertools\n","from sklearn.model_selection import cross_val_score\n","from sklearn.ensemble import RandomForestClassifier\n","import pandas as pd\n","\n","# Load the CSV file into a DataFrame\n","data = pd.read_csv('/content/drive/MyDrive/Chapter5Model//merged_CSV/merged_162K_Chapter5_Five_Class_Model.csv').dropna()\n","\n","n = 1000 # Define sample size\n","grouped = data.groupby('Class') # Group by class\n","sampled = grouped.apply(lambda x: x.sample(n=n, replace=True).reset_index(drop=True)) # Sample from each group\n","sampled = [sampled] # Convert to list\n","data = pd.concat(sampled)  # Concatenate samples\n","\n","target = data['Class']\n","data = data.drop(['Class','.geo','system:index','City', 'NDBI', 'UI', 'SwiRed', 'INDBI', 'NBAI', 'BLFEI'], axis=1)\n","\n","\n","# Assuming 'data' is your DataFrame and 'target' is the target variable\n","features = data.columns.tolist()\n","\n","# Record the best accuracy and feature combination\n","best_accuracy = 0\n","best_combination = None\n","\n","# Loop over all possible combinations of features\n","for r in range(1, len(features) + 1):\n","    for combination in itertools.combinations(features, r):\n","        # Train the model using cross-validation\n","        model = RandomForestClassifier()\n","        scores = cross_val_score(model, data[list(combination)], target, cv=5)\n","\n","        # Calculate average accuracy\n","        accuracy = scores.mean()\n","\n","        # If this combination has the best accuracy so far, record it\n","        if accuracy > best_accuracy:\n","            best_accuracy = accuracy\n","            best_combination = combination\n","    print(f\"Combination: {best_combination}\",f\"Accuracy: {best_accuracy}\")\n","\n","print(f\"Best accuracy: {best_accuracy}\")\n","print(f\"Best feature combination: {best_combination}\")\n"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1gWX8GMpIf4Ohoy8qst4I4g_K1YyozBzE","authorship_tag":"ABX9TyN10vfHW5UowI2WVfUkYSP7"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}